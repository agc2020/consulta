<!DOCTYPE html>

<html lang="pt-BR">
<head>
<meta charset="utf-8"/>
<title>ResoluÃ§Ã£o CNJ NÂº 615/2025 â€“ Espelho</title>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="noindex" name="robots"/>
<meta content="Espelho do Ato Normativo - TJPR (espelho tÃ©cnico para IA)" name="description"/>
<meta content="CNJ" data-pagefind-filter="orgao[content]"/>
<meta content="ResoluÃ§Ã£o" data-pagefind-filter="tipo[content]"/>
<meta content="2025" data-pagefind-filter="ano[content]" data-pagefind-sort="ano[content]"/>
<style>
 /* VariÃ¡veis CSS para temas */
 :root {
 --primary-color: #0066cc;
 --text-color: #333;
 --text-muted: #666;
 --bg-body: #f8f9fa;
 --bg-container: white;
 --bg-meta: #f8f9fa;
 --border-color: #e9ecef;
 --border-header: #0066cc;
 --shadow: 0 0 10px rgba(0,0,0,0.1);
 --disclaimer-bg: #fff3cd;
 --disclaimer-border: #ffeaa7;
 --disclaimer-text: #856404;
 }

 /* Modo dark */
 [data-theme="dark"] {
 --primary-color: #4a9eff;
 --text-color: #e0e0e0;
 --text-muted: #a0a0a0;
 --bg-body: #1a1a1a;
 --bg-container: #252525;
 --bg-meta: #2a2a2a;
 --border-color: #333;
 --border-header: #4a9eff;
 --shadow: 0 0 10px rgba(0,0,0,0.5);
 --disclaimer-bg: #3d3416;
 --disclaimer-border: #5a4a1f;
 --disclaimer-text: #f4d06f;
 }

 * {
 margin: 0;
 padding: 0;
 box-sizing: border-box;
 }

 body {
 font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
 line-height: 1.6;
 color: var(--text-color);
 background-color: var(--bg-body);
 transition: background-color 0.3s ease, color 0.3s ease;
 }

 .container {
 max-width: 900px;
 margin: 0 auto;
 /* increase horizontal padding to give more breathing room on the left and right */
 padding: 20px 40px;
 background-color: var(--bg-container);
 box-shadow: var(--shadow);
 min-height: 100vh;
 transition: background-color 0.3s ease, box-shadow 0.3s ease;
 }

 header {
 border-bottom: 3px solid var(--border-header);
 padding-bottom: 20px;
 margin-bottom: 0;
 transition: border-color 0.3s ease;
 }

 header h1 {
 color: var(--primary-color);
 font-size: 1.8em;
 margin-bottom: 10px;
 font-weight: 600;
 transition: color 0.3s ease;
 }

 .subtitle {
 color: var(--text-muted);
 font-size: 1.1em;
 font-style: italic;
 transition: color 0.3s ease;
 }


 /* Estilos do toggle de tema */
 .theme-toggle-container {
 display: flex;
 flex-direction: row;
 align-items: center;
 justify-content: flex-end;
 gap: 6px;
 background: transparent;
 padding: 8px 0;
 margin-bottom: 8px;
 border-radius: 0;
 box-shadow: none;
 }

 .theme-toggle-label {
 font-size: 0.65rem;
 font-weight: 500;
 color: var(--text-muted);
 margin-right: 3px;
 }

 .theme-options {
 display: flex;
 gap: 3px;
 }

 .theme-option {
 display: flex;
 align-items: center;
 justify-content: center;
 gap: 2px;
 padding: 2px 5px;
 border: 1px solid var(--border-color);
 border-radius: 4px;
 background: transparent;
 color: var(--text-muted);
 font-size: 0.6rem;
 cursor: pointer;
 transition: all .2s ease;
 white-space: nowrap;
 opacity: 0.6;
 }

 .theme-option:hover {
 opacity: 1;
 border-color: var(--primary-color);
 background: var(--bg-meta);
 }

 .theme-option.active {
 opacity: 1;
 border-color: var(--primary-color);
 background: var(--primary-color);
 color: white;
 font-weight: 500;
 }

 .theme-option input[type="radio"] {
 display: none;
 }

 .theme-icon {
 font-size: 0.7rem;
 }

 .meta {
 background-color: var(--bg-meta);
 border: 1px solid var(--border-color);
 border-radius: 5px;
 padding: 15px;
 margin-bottom: 30px;
 font-size: 0.9em;
 transition: background-color 0.3s ease, border-color 0.3s ease;
 }

 .meta div {
 margin-bottom: 5px;
 font-size: 1.2em;
 }

 .meta strong {
 color: var(--text-color);
 transition: color 0.3s ease;
 }

 .meta a {
 color: var(--primary-color);
 text-decoration: none;
 transition: color 0.3s ease;
 }

 .meta a:hover {
 text-decoration: underline;
 }

 article {
 line-height: 1.8;
 }

 article h1 {
 color: var(--primary-color);
 font-size: 1.4em;
 margin: 20px 0 12px 0;
 /* use a real font weight instead of the invalid 'justify' keyword */
 font-weight: 600;
 text-align: justify;
 transition: color 0.3s ease;
 }

 article h2 {
 /* Estilo unificado para seÃ§Ãµes (CapÃ­tulos, SeÃ§Ãµes, SubseÃ§Ãµes) */
 color: var(--text-color);
 font-size: 1.4rem;
 font-weight: normal;
 text-align: justify;
 line-height: 1.3;
 margin: 20px 0 12px 0;
 }

 article h3 {
 /* Estilo unificado para artigos e demais nÃ­veis normativos */
 color: var(--text-color);
 font-size: 1.4rem;
 font-weight: normal;
 text-align: justify;
 line-height: 1.3;
 margin: 20px 0 12px 0;
 }

 article h4 {
 /* Aplicar mesmo estilo de h3 para parÃ¡grafos */
 color: var(--text-color);
 font-size: 1.4rem;
 font-weight: normal;
 text-align: justify;
 line-height: 1.3;
 margin: 20px 0 12px 0;
 }

 article h5 {
 /* Aplicar mesmo estilo de h3 para incisos */
 color: var(--text-color);
 font-size: 1.4rem;
 font-weight: normal;
 text-align: justify;
 line-height: 1.3;
 margin: 20px 0 12px 0;
 }

 article h6 {
 /* Aplicar mesmo estilo de h3 para alÃ­neas */
 color: var(--text-color);
 font-size: 1.4rem;
 font-weight: normal;
 text-align: justify;
 line-height: 1.3;
 margin: 20px 0 12px 0;
 }

 article p {
 /* ParÃ¡grafos comuns seguem o estilo unificado dos tÃ­tulos normativos */
 color: var(--text-color);
 font-size: 1.4rem;
 font-weight: normal;
 text-align: justify;
 line-height: 1.3;
 margin: 20px 0 12px 0;
 }

 /* enlarge the very first paragraph inside the article, which typically contains the document's
 title (e.g. "INSTRUÃ‡ÃƒO NORMATIVA NÂº ..."), making it stand out. */
 article p:first-of-type {
 font-size: 1.6em;
 line-height: 1.4;
 font-weight: 700;
 text-align: center;
 color: var(--primary-color);
 margin: 24px 0;
 transition: color 0.3s ease;
 }

 footer {
 margin-top: 40px;
 padding-top: 20px;
 border-top: 1px solid var(--border-color);
 transition: border-color 0.3s ease;
 }

 .disclaimer {
 background-color: var(--disclaimer-bg);
 border: 1px solid var(--disclaimer-border);
 padding: 15px;
 border-radius: 5px;
 font-size: 1.2em;
 color: var(--disclaimer-text);
 text-align: justify;
 transition: all 0.3s ease;
 text-align: justify;
 }

 .disclaimer strong {
 color: var(--disclaimer-text);
 transition: color 0.3s ease;
 }

 .disclaimer a {
 color: var(--disclaimer-text);
 text-decoration: underline;
 transition: color 0.3s ease;
 }

 @media (max-width: 768px) {
 .container {
 padding: 15px;
 margin: 0;
 box-shadow: none;
 }
 
 header h1 {
 font-size: 1.5em;
 }
 
 article h1 {
 font-size: 1.3em;
 }
 
 .theme-toggle-container {
 justify-content: center;
 padding: 10px 0;
 }
 
 .theme-toggle-label {
 font-size: 0.7rem;
 }
 
 .theme-option {
 padding: 3px 6px;
 font-size: 0.65rem;
 }
 
 .theme-icon {
 font-size: 0.75rem;
 }
 }
 </style>
</head>
<body>
<main class="container">
<header>
<h1>ResoluÃ§Ã£o CNJ NÂº 615/2025</h1>
<p class="subtitle">Clique no hiperlink ABAIXO para consultar o conteÃºdo diretamente na fonte oficial.</p>
</header>
<!-- Toggle de Tema -->
<div class="theme-toggle-container">
<div class="theme-toggle-label">Tema</div>
<div class="theme-options">
<label class="theme-option" data-theme="auto">
<input checked="" name="theme" type="radio" value="auto"/>
<span class="theme-icon">ğŸŒ“</span>
<span>Sistema</span>
</label>
<label class="theme-option" data-theme="light">
<input name="theme" type="radio" value="light"/>
<span class="theme-icon">â˜€ï¸</span>
<span>Claro</span>
</label>
<label class="theme-option" data-theme="dark">
<input name="theme" type="radio" value="dark"/>
<span class="theme-icon">ğŸŒ™</span>
<span>Escuro</span>
</label>
</div>
</div>
<div class="meta">
<div><strong>Ãšltima sincronizaÃ§Ã£o:</strong> 16/11/2025 22:38:29 (processado via snapshot.py)</div>
<div><strong>Fonte oficial:</strong> <a href="https://atos.cnj.jus.br/atos/detalhar/6001" rel="nofollow" target="_blank">CNJ â€“ Atos Normativos</a></div>
<div><strong>MÃ©todo de extraÃ§Ã£o:</strong> snapshot.py (Playwright + BeautifulSoup + FormataÃ§Ã£o)</div>
<div><strong>Tamanho do conteÃºdo:</strong> origem: 80.868 caracteres | espelho: 80.353 caracteres</div>
<div><strong>ObservaÃ§Ã£o:</strong> Este espelho existe apenas para facilitar consultas por agentes de IA. Para uso jurÃ­dico, sempre consulte a fonte oficial.</div>
</div>
<article>
<p>ResoluÃ§Ã£o NÂº 615 de 11/03/2025</p>
<p>Tecnologia Da InformaÃ§Ã£o E ComunicaÃ§Ã£o; Funcionamento dos Ã“rgÃ£os Judiciais; GestÃ£o da InformaÃ§Ã£o e de Demandas Judiciais; GestÃ£o e OrganizaÃ§Ã£o JudiciÃ¡ria; <div class="row"> <div class="col-2 identificacao">Assuntos</div> <div class="col-10 dsc_conteudo"> </div> </div></p>
<p>Ementa</p>
<p>Estabelece diretrizes para o desenvolvimento, utilizaÃ§Ã£o e governanÃ§a de soluÃ§Ãµes desenvolvidas com recursos de inteligÃªncia artificial no Poder JudiciÃ¡rio.</p>
<p>Fonte: DJe/CNJ n. 54/2025, de 14 de marÃ§o de 2025, p. 2-17.</p>
<p>O PRESIDENTE DO CONSELHO NACIONAL DE JUSTIÃ‡A (CNJ), no uso de suas atribuiÃ§Ãµes legais e regimentais,</p>
<h2>CONSIDERANDO que a ResoluÃ§Ã£o CNJ nÂº 332/2020 , estabelece diretrizes sobre a Ã©tica, a transparÃªncia e a governanÃ§a na produÃ§Ã£o e no uso de inteligÃªncia artificial no Poder JudiciÃ¡rio;</h2>
<h2>CONSIDERANDO o acelerado desenvolvimento de tecnologias de inteligÃªncia artificial, notadamente por meio de algoritmos que utilizam grandes modelos de linguagem, os quais sÃ£o capazes de interagir com usuÃ¡rios e oferecer soluÃ§Ãµes geradas automaticamente;</h2>
<h2>CONSIDERANDO a imprescindibilidade de regulamentaÃ§Ã£o especÃ­fica para o emprego de tÃ©cnicas de inteligÃªncia artificial generativa no Ã¢mbito do Poder JudiciÃ¡rio, com plena transparÃªncia e publicidade, de modo a assegurar que sua utilizaÃ§Ã£o esteja em consonÃ¢ncia com valores Ã©ticos fundamentais, incluindo dignidade humana, respeito aos direitos humanos, nÃ£o discriminaÃ§Ã£o, devido processo, devida motivaÃ§Ã£o e fundamentaÃ§Ã£o da prestaÃ§Ã£o da atividade jurisdicional, prestaÃ§Ã£o de contas e responsabilizaÃ§Ã£o;</h2>
<h2>CONSIDERANDO a importÃ¢ncia de promover a autonomia dos tribunais na adoÃ§Ã£o de tecnologias inovadoras, incentivando prÃ¡ticas que garantam a inovaÃ§Ã£o Ã©tica, responsÃ¡vel e segura no uso da inteligÃªncia artificial;</h2>
<h2>CONSIDERANDO os potenciais riscos associados Ã  utilizaÃ§Ã£o de inteligÃªncia artificial generativa, incluindo ameaÃ§as Ã  soberania nacional, Ã  seguranÃ§a da informaÃ§Ã£o, Ã  privacidade e proteÃ§Ã£o de dados pessoais, bem como a possibilidade de intensificaÃ§Ã£o de parcialidades e vieses discriminatÃ³rios;</h2>
<h2>CONSIDERANDO que o uso da inteligÃªncia artificial generativa em auxÃ­lio Ã  produÃ§Ã£o de decisÃµes judiciais exige transparÃªncia e a necessÃ¡ria fiscalizaÃ§Ã£o, revisÃ£o e intervenÃ§Ã£o humana da magistratura;</h2>
<h2>CONSIDERANDO que a ResoluÃ§Ã£o CNJ nÂº 332/2020 foi formulada tendo como foco as soluÃ§Ãµes computacionais destinadas a auxiliar na gestÃ£o processual e na efetividade da prestaÃ§Ã£o jurisdicional disponÃ­veis Ã  Ã©poca de sua elaboraÃ§Ã£o, e que agora se faz necessÃ¡rio atualizar esse normativo para abarcar novas tecnologias, em especial aquelas conhecidas como inteligÃªncias artificiais generativas;</h2>
<h2>CONSIDERANDO o parecer oferecido pela ComissÃ£o Permanente de Tecnologia da InformaÃ§Ã£o e InovaÃ§Ã£o do Conselho Nacional de JustiÃ§a no Procedimento de Controle Administrativo de autos nÂº 0000416-89.2023.2.00.0000, que destacou a importÃ¢ncia da governanÃ§a adequada no uso de inteligÃªncia artificial, em particular a generativa, no Poder JudiciÃ¡rio;</h2>
<h2>CONSIDERANDO a necessidade de assegurar que o desenvolvimento e a implantaÃ§Ã£o de modelos de inteligÃªncia artificial no Poder JudiciÃ¡rio observem critÃ©rios Ã©ticos de transparÃªncia, previsibilidade, auditabilidade e justiÃ§a substancial;</h2>
<h2>CONSIDERANDO que as soluÃ§Ãµes de inteligÃªncia artificial devem ser auditadas sob a Ã³tica da seguranÃ§a da informaÃ§Ã£o, proteÃ§Ã£o de dados, performance, robustez, confiabilidade, prevenÃ§Ã£o de vieses discriminatÃ³rios, correlaÃ§Ã£o entre entradas e saÃ­das e</h2>
<p>conformidade legal e Ã©tica;</p>
<h2>CONSIDERANDO a relevÃ¢ncia de fomentar a colaboraÃ§Ã£o e o compartilhamento de informaÃ§Ãµes sobre o uso de inteligÃªncia artificial no Poder JudiciÃ¡rio, com vistas a assegurar a transparÃªncia e eficÃ¡cia na aplicaÃ§Ã£o dessas tecnologias;</h2>
<h2>CONSIDERANDO a necessidade de respeitar as prerrogativas do MinistÃ©rio PÃºblico, da Defensoria PÃºblica, da Advocacia e dos demais atores do sistema de justiÃ§a;</h2>
<h2>CONSIDERANDO as sugestÃµes recolhidas de magistrados e demais atores do sistema de justiÃ§a, da sociedade civil, de especialistas e de instituiÃ§Ãµes pÃºblicas e privadas para a atualizaÃ§Ã£o da ResoluÃ§Ã£o CNJ nÂº 332/2020 durante audiÃªncia pÃºblica ocorrida entre os dias 25 e 27 de setembro de 2024;</h2>
<h2>CONSIDERANDO o relatÃ³rio do Grupo de Trabalho sobre InteligÃªncia Artificial no Poder JudiciÃ¡rio, instituÃ­do pela Portaria CNJ nÂº 338/2023 , cujo objetivo Ã© realizar estudos e apresentar propostas para a regulamentaÃ§Ã£o do uso de sistemas de inteligÃªncia artificial generativa;</h2>
<h2>CONSIDERANDO a decisÃ£o proferida pelo PlenÃ¡rio do CNJ no julgamento do Procedimento de Ato Normativo de autos nÂº 0000563-47.2025.2.00.0000 na 1Âª SessÃ£o ExtraordinÃ¡ria de 2025, realizada em 18 de fevereiro de 2025;</h2>
<h2>CONSIDERANDO :</h2>
<h2>CAPÃTULO I</h2>
<h3>DAS DEFINIÃ‡Ã•ES E FUNDAMENTOS PARA O USO DE SOLUÃ‡Ã•ES DE IA NO PODER JUDICIÃRIO</h3>
<h3>Art. 1Âº A presente ResoluÃ§Ã£o estabelece normas para o desenvolvimento, a governanÃ§a, a auditoria, o monitoramento e o uso responsÃ¡vel de soluÃ§Ãµes que adotam tÃ©cnicas de inteligÃªncia artificial (IA) no Ã¢mbito do Poder JudiciÃ¡rio, com o objetivo de promover a inovaÃ§Ã£o tecnolÃ³gica e a eficiÃªncia dos serviÃ§os judiciÃ¡rios de modo seguro, transparente, isonÃ´mico e Ã©tico, em benefÃ­cio dos jurisdicionados e com estrita observÃ¢ncia de seus direitos fundamentais.</h3>
<h4>Â§ 1Âº A governanÃ§a das soluÃ§Ãµes de IA deverÃ¡ respeitar a autonomia dos tribunais, permitindo o desenvolvimento e a implementaÃ§Ã£o de soluÃ§Ãµes inovadoras locais, ajustando-se aos contextos especÃ­ficos de cada tribunal, desde que observados os padrÃµes de auditoria, monitoramento e transparÃªncia definidos por esta ResoluÃ§Ã£o, sem prejuÃ­zo da atuaÃ§Ã£o do CNJ, no Ã¢mbito de suas competÃªncias.</h4>
<h4>Â§ 2Âº A auditoria e o monitoramento das soluÃ§Ãµes de IA serÃ£o realizados com base em critÃ©rios proporcionais ao impacto da soluÃ§Ã£o, garantindo que os sistemas sejam auditÃ¡veis ou monitorÃ¡veis de forma prÃ¡tica e acessÃ­vel, sem a obrigatoriedade de acesso irrestrito ao cÃ³digo-fonte, desde que sejam adotados mecanismos de transparÃªncia e controle sobre o uso dos dados e as decisÃµes automatizadas.</h4>
<h4>Â§ 3Âº A transparÃªncia no uso de IA serÃ¡ promovida por meio de indicadores claros e relatÃ³rios pÃºblicos, que informem o uso dessas soluÃ§Ãµes de maneira compreensÃ­vel e em linguagem simples, garantindo que os jurisdicionados tenham ciÃªncia do uso de IA, quando aplicÃ¡vel, sem que isso prejudique a eficiÃªncia ou credibilidade dos processos e decisÃµes judiciais.</h4>
<h4>Â§ 4Âº Os tribunais deverÃ£o priorizar o desenvolvimento colaborativo de soluÃ§Ãµes de IA, promovendo a interoperabilidade e a disseminaÃ§Ã£o de tecnologias, cÃ³digos, bases de dados e boas prÃ¡ticas com outros Ã³rgÃ£os do Poder JudiciÃ¡rio.</h4>
<h4>Â§ 5Âº O CNJ poderÃ¡ criar mecanismos de incentivo, tais como reconhecimento pÃºblico, premiaÃ§Ãµes ou priorizaÃ§Ã£o de recursos e investimentos em inovaÃ§Ã£o, para tribunais que, dentre outros critÃ©rios previstos em regulamento, adotem prÃ¡ticas colaborativas/cooperativas no desenvolvimento de soluÃ§Ãµes de IA.</h4>
<h3>Art. 2Âº O desenvolvimento, a governanÃ§a, a auditoria, o monitoramento e o uso responsÃ¡vel de soluÃ§Ãµes de IA pelo Poder JudiciÃ¡rio tÃªm como fundamentos:</h3>
<h5>I â€“ o respeito aos direitos fundamentais e aos valores democrÃ¡ticos;</h5>
<h5>II â€“ a promoÃ§Ã£o do bem-estar dos jurisdicionados;</h5>
<h5>III â€“ o desenvolvimento tecnolÃ³gico e o estÃ­mulo Ã  inovaÃ§Ã£o no setor pÃºblico, com Ãªnfase na colaboraÃ§Ã£o entre os tribunais e o CNJ para o incremento da eficiÃªncia dos serviÃ§os judiciÃ¡rios, respeitada a autonomia dos tribunais para o desenvolvimento de soluÃ§Ãµes que atendam Ã s suas necessidades especÃ­ficas;</h5>
<h5>IV â€“ a centralidade da pessoa humana;</h5>
<h5>V â€“ a participaÃ§Ã£o e a supervisÃ£o humana em todas as etapas dos ciclos de desenvolvimento e de utilizaÃ§Ã£o das soluÃ§Ãµes que adotem tÃ©cnicas de inteligÃªncia artificial, ressalvado o uso dessas tecnologias como ferramentas auxiliares para aumentar a eficiÃªncia e automaÃ§Ã£o de serviÃ§os judiciÃ¡rios meramente acessÃ³rios ou procedimentais e para suporte Ã  decisÃ£o;</h5>
<h5>VI â€“ a promoÃ§Ã£o da igualdade, da pluralidade e da justiÃ§a decisÃ³ria;</h5>
<h5>VII â€“ a formulaÃ§Ã£o de soluÃ§Ãµes seguras para os usuÃ¡rios internos e externos, com a identificaÃ§Ã£o, a classificaÃ§Ã£o, o monitoramento e a mitigaÃ§Ã£o de riscos sistÃªmicos;</h5>
<h5>VIII â€“ a proteÃ§Ã£o de dados pessoais, o acesso Ã  informaÃ§Ã£o e o respeito ao segredo de justiÃ§a;</h5>
<h5>IX â€“ a curadoria dos dados usados no desenvolvimento e no aprimoramento de inteligÃªncia artificial, adotando fontes de dados seguras, rastreÃ¡veis e auditÃ¡veis, preferencialmente governamentais, permitida a contrataÃ§Ã£o de fontes privadas, desde que atendam aos requisitos de seguranÃ§a e auditabilidade estabelecidos nesta ResoluÃ§Ã£o ou pelo ComitÃª Nacional de InteligÃªncia Artificial do JudiciÃ¡rio;</h5>
<h5>X â€“ a conscientizaÃ§Ã£o e a difusÃ£o do conhecimento sobre as soluÃ§Ãµes que adotam tÃ©cnicas de inteligÃªncia artificial, com capacitaÃ§Ã£o contÃ­nua dos seus usuÃ¡rios sobre as suas aplicaÃ§Ãµes, os seus mecanismos de funcionamento e os seus riscos;</h5>
<h5>XI â€“ a garantia da seguranÃ§a da informaÃ§Ã£o e da seguranÃ§a cibernÃ©tica; e</h5>
<h5>XII â€“ a transparÃªncia dos relatÃ³rios de auditoria, de avaliaÃ§Ã£o de impacto algorÃ­tmico e monitoramento.</h5>
<h3>Art. 3Âº O desenvolvimento, a governanÃ§a, a auditoria, o monitoramento e o uso responsÃ¡vel de soluÃ§Ãµes de IA pelos tribunais tÃªm como princÃ­pios:</h3>
<h5>I â€“ a justiÃ§a, a equidade, a inclusÃ£o e a nÃ£o-discriminaÃ§Ã£o abusiva ou ilÃ­cita;</h5>
<h5>II â€“ a transparÃªncia, a eficiÃªncia, a explicabilidade, a contestabilidade, a auditabilidade e a confiabilidade das soluÃ§Ãµes que adotam tÃ©cnicas de inteligÃªncia artificial;</h5>
<h5>III â€“ a seguranÃ§a jurÃ­dica e a seguranÃ§a da informaÃ§Ã£o;</h5>
<h5>IV â€“ a busca da eficiÃªncia e qualidade na entrega da prestaÃ§Ã£o jurisdicional pelo Poder JudiciÃ¡rio, garantindo sempre a observÃ¢ncia dos direitos fundamentais;</h5>
<h5>V â€“ o devido processo legal, a ampla defesa e o contraditÃ³rio, a identidade fÃ­sica do juiz e a razoÃ¡vel duraÃ§Ã£o do processo, com observÃ¢ncia das prerrogativas e dos direitos dos atores do sistema de JustiÃ§a;</h5>
<h5>VI â€“ a prevenÃ§Ã£o, a precauÃ§Ã£o e o controle quanto a medidas eficazes para a mitigaÃ§Ã£o de riscos derivados do uso intencional ou nÃ£o intencional de soluÃ§Ãµes que adotam tÃ©cnicas de inteligÃªncia artificial;</h5>
<h5>VII â€“ a supervisÃ£o humana efetiva, periÃ³dica e adequada no ciclo de vida da inteligÃªncia artificial, considerando o grau de risco envolvido, com possibilidade de ajuste dessa supervisÃ£o conforme o nÃ­vel de automaÃ§Ã£o e impacto da soluÃ§Ã£o utilizada; e</h5>
<h5>VIII â€“ a oferta, pelos tribunais e suas escolas, de capacitaÃ§Ã£o contÃ­nua para magistrados e servidores sobre riscos da automaÃ§Ã£o, vieses algorÃ­tmicos e anÃ¡lise crÃ­tica dos resultados gerados por IA.</h5>
<h3>Art. 4Âº Para o disposto nesta ResoluÃ§Ã£o, consideram-se:</h3>
<h5>I â€“ sistema de inteligÃªncia artificial (IA): sistema baseado em mÃ¡quina que, com diferentes nÃ­veis de autonomia e para objetivos explÃ­citos ou implÃ­citos, processa um conjunto de dados ou informaÃ§Ãµes fornecido e com o objetivo de gerar resultados provÃ¡veis e coerentes de</h5>
<p>decisÃ£o, recomendaÃ§Ã£o ou conteÃºdo, que possam influenciar o ambiente virtual, fÃ­sico ou real;</p>
<h5>II â€“ ciclo de vida: sÃ©rie de fases que compreende a concepÃ§Ã£o, o planejamento, o desenvolvimento, o treinamento, o retreinamento, a testagem, a validaÃ§Ã£o, a implantaÃ§Ã£o, o monitoramento e eventuais modificaÃ§Ãµes e adaptaÃ§Ãµes de um sistema de inteligÃªncia artificial, incluindo sua descontinuidade, que pode ocorrer em quaisquer das etapas referidas, e o acompanhamento de seus impactos apÃ³s a</h5>
<p>implantaÃ§Ã£o;</p>
<h5>III â€“ Sinapses: soluÃ§Ã£o computacional destinada a armazenar, testar, treinar, distribuir e auditar modelos de inteligÃªncia artificial, disponÃ­vel na Plataforma Digital do Poder JudiciÃ¡rio (PDPJ-Br);</h5>
<h5>IV â€“ desenvolvedor de sistema de inteligÃªncia artificial: pessoa natural ou jurÃ­dica, de natureza pÃºblica ou privada, que desenvolva ou comissione um sistema de inteligÃªncia artificial, com a finalidade de colocÃ¡-lo no mercado ou aplicÃ¡-lo em serviÃ§o fornecido, sob seu prÃ³prio nome ou marca, a tÃ­tulo oneroso ou gratuito;</h5>
<h5>V â€“ usuÃ¡rio: pessoa que utiliza o sistema de IA e exerce controle sobre suas funcionalidades, podendo tal controle ser regulado ou limitado conforme seja externo ou interno ao Poder JudiciÃ¡rio;</h5>
<h5>VI â€“ usuÃ¡rio interno: membro, servidor ou colaborador do Poder JudiciÃ¡rio que desenvolva ou utilize o sistema inteligente, podendo ser enquadrado em diferentes perfis conforme o cargo e Ã¡rea de atuaÃ§Ã£o;</h5>
<h5>VII â€“ usuÃ¡rio externo: pessoa externa ao Poder JudiciÃ¡rio, que interage diretamente com o sistema de IA do JudiciÃ¡rio, incluindo advogados, defensores pÃºblicos, procuradores, membros do MinistÃ©rio PÃºblico, peritos, assistentes tÃ©cnicos e jurisdicionados em geral;</h5>
<h5>VIII â€“ distribuidor: pessoa natural ou jurÃ­dica, de natureza pÃºblica ou privada, que disponibiliza e distribui sistema de IA para que terceiro o opere a tÃ­tulo oneroso ou gratuito;</h5>
<h5>IX â€“ inteligÃªncia artificial generativa (IA generativa ou IAGen): sistema de IA especificamente destinado a gerar ou modificar significativamente, com diferentes nÃ­veis de autonomia, texto, imagens, Ã¡udio, vÃ­deo ou cÃ³digo de software, alÃ©m dos modelos estatÃ­sticos e de aprendizado a partir dos dados treinados;</h5>
<h5>X â€“ avaliaÃ§Ã£o preliminar: processo de avaliaÃ§Ã£o de um sistema de IA, pelo tribunal desenvolvedor ou contratante, antes de sua utilizaÃ§Ã£o ou entrada em produÃ§Ã£o na PDPJ-Br, com o objetivo de classificar seu grau de risco e atender Ã s obrigaÃ§Ãµes estabelecidas nesta ResoluÃ§Ã£o;</h5>
<h5>XI â€“ avaliaÃ§Ã£o de impacto algorÃ­tmico: anÃ¡lise contÃ­nua dos impactos de um sistema de IA sobre os direitos fundamentais, com a identificaÃ§Ã£o de medidas preventivas, mitigadoras de danos e de maximizaÃ§Ã£o dos impactos positivos, sem a violaÃ§Ã£o da propriedade industrial e intelectual da soluÃ§Ã£o de IA utilizada;</h5>
<h5>XII â€“ ComitÃª Nacional de InteligÃªncia Artificial do JudiciÃ¡rio: comitÃª com composiÃ§Ã£o plural que tem por finalidade auxiliar o CNJ na implementaÃ§Ã£o, no cumprimento e na supervisÃ£o da aplicaÃ§Ã£o desta ResoluÃ§Ã£o, sempre mediante diÃ¡logo com os tribunais e a sociedade civil;</h5>
<h5>XIII â€“ viÃ©s discriminatÃ³rio ilegal ou abusivo: resultado indevidamente discriminatÃ³rio que cria, reproduz ou reforÃ§a preconceitos ou tendÃªncias, derivados ou nÃ£o dos dados ou seu treinamento;</h5>
<h5>XIV â€“ privacyby design: preservaÃ§Ã£o da privacidade dos dados desde a concepÃ§Ã£o de qualquer novo projeto ou serviÃ§o de IA durante todo o seu ciclo de vida, inclusive na anonimizaÃ§Ã£o e encriptaÃ§Ã£o de dados sigilosos;</h5>
<h5>XV â€“ privacyby default: utilizaÃ§Ã£o, por padrÃ£o, de alto nÃ­vel de confidencialidade de dados;</h5>
<h5>XVI â€“ prompt: texto em linguagem natural utilizado na IA generativa para execuÃ§Ã£o de uma tarefa especÃ­fica;</h5>
<h5>XVII â€“ auditabilidade: capacidade de um sistema de IA se sujeitar Ã  avaliaÃ§Ã£o dos seus algoritmos, dados, processos de</h5>
<p>concepÃ§Ã£o ou resultados, sempre que tecnicamente possÃ­vel;</p>
<h5>XVIII â€“ explicabilidade: compreensÃ£o clara, sempre que tecnicamente possÃ­vel, de como as â€œdecisÃµesâ€ sÃ£o tomadas pela IA; e</h5>
<h5>XIX â€“ contestabilidade: possibilidade de questionamento e revisÃ£o dos resultados gerados pela IA.</h5>
<h2>CAPÃTULO II</h2>
<h3>DO RESPEITO AOS DIREITOS FUNDAMENTAIS</h3>
<h3>Art. 5Âº No desenvolvimento, na implantaÃ§Ã£o e no uso de soluÃ§Ãµes de inteligÃªncia artificial no JudiciÃ¡rio, os tribunais observarÃ£o a compatibilidade dessas soluÃ§Ãµes com os direitos fundamentais, especialmente aqueles previstos na ConstituiÃ§Ã£o da RepÃºblica ou em tratados de que a RepÃºblica Federativa do Brasil seja parte.</h3>
<h4>Â§ 1Âº A verificaÃ§Ã£o de compatibilidade com os direitos fundamentais deverÃ¡ ocorrer em todas as fases do ciclo de vida da soluÃ§Ã£o de IA, incluindo o desenvolvimento, a implantaÃ§Ã£o, o uso, as atualizaÃ§Ãµes e eventuais retreinamentos dos sistemas e seus dados.</h4>
<h4>Â§ 2Âº Os tribunais deverÃ£o implementar mecanismos de auditoria e monitoramento contÃ­nuos, com vistas a garantir que as soluÃ§Ãµes de IA permaneÃ§am em conformidade com os direitos fundamentais, e proceder a ajustes sempre que forem identificadas incompatibilidades.</h4>
<h4>Â§ 3Âº Havendo notÃ­cia ou indÃ­cios de violaÃ§Ã£o a direitos fundamentais, assegura-se Ã  Ordem dos Advogados do Brasil (OAB), ao MinistÃ©rio PÃºblico e demais entidades legitimadas o acesso Ã s avaliaÃ§Ãµes de impacto algorÃ­tmico e o direito de peticionar ao ComitÃª para que seja avaliada a necessidade de solicitaÃ§Ã£o de auditorias e outras formas de controle.</h4>
<h3>Art. 6Âº A adoÃ§Ã£o de aplicaÃ§Ãµes que utilizem modelos de inteligÃªncia artificial deve buscar garantir a seguranÃ§a jurÃ­dica e colaborar para que o Poder JudiciÃ¡rio respeite os princÃ­pios previstos no art. 3Âº desta ResoluÃ§Ã£o.</h3>
<h4>ParÃ¡grafo Ãºnico. Os tribunais e desenvolvedores de IA serÃ£o responsÃ¡veis pela criaÃ§Ã£o de diretrizes internas para assegurar que as soluÃ§Ãµes de IA estejam em conformidade com os princÃ­pios estabelecidos no art. 3Âº desta ResoluÃ§Ã£o, com mecanismos adequados de supervisÃ£o e revisÃ£o periÃ³dica.</h4>
<h3>Art. 7Âº Os dados utilizados no desenvolvimento ou treinamento de modelos de inteligÃªncia artificial devem ser representativos de casos judiciais e observar as cautelas necessÃ¡rias quanto ao segredo de justiÃ§a e Ã  proteÃ§Ã£o de dados pessoais, nos termos da Lei nÂº 13.709/2018 (Lei Geral de ProteÃ§Ã£o de Dados Pessoais â€“ LGPD) .</h3>
<h4>Â§ 1Âº Consideram-se dados representativos aqueles que refletem de forma adequada a diversidade de situaÃ§Ãµes e contextos presentes no Poder JudiciÃ¡rio, evitando vieses que possam comprometer a equidade e a justiÃ§a decisÃ³ria.</h4>
<h4>Â§ 2Âº Os dados deverÃ£o ser anonimizados sempre que possÃ­vel, providÃªncia obrigatÃ³ria para os dados sigilosos ou protegidos por segredo de justiÃ§a, de acordo com as melhores prÃ¡ticas de proteÃ§Ã£o de dados e seguranÃ§a da informaÃ§Ã£o.</h4>
<h4>Â§ 3Âº Os tribunais deverÃ£o implementar mecanismos de curadoria e monitoramento dos dados utilizados, assegurando a conformidade com a legislaÃ§Ã£o de proteÃ§Ã£o de dados e a revisÃ£o periÃ³dica das prÃ¡ticas de tratamento de dados.</h4>
<h3>Art. 8Âº Os produtos gerados pela inteligÃªncia artificial para suporte Ã s decisÃµes judiciais deverÃ£o preservar a igualdade, a nÃ£o discriminaÃ§Ã£o abusiva ou ilÃ­cita e a pluralidade, assegurando que os sistemas de IA auxiliem no julgamento justo e contribuam para eliminar ou minimizar a marginalizaÃ§Ã£o do ser humano e os erros de julgamento decorrentes de preconceitos</h3>
<h4>Â§ 1Âº DeverÃ£o ser implementadas medidas preventivas para evitar o surgimento de vieses discriminatÃ³rios, incluindo a validaÃ§Ã£o contÃ­nua das soluÃ§Ãµes de IA e a auditoria ou monitoramento de suas decisÃµes ao longo de todo o ciclo de vida da aplicaÃ§Ã£o, para garantir que as soluÃ§Ãµes de IA continuem em conformidade com os princÃ­pios da igualdade, pluralidade e nÃ£o discriminaÃ§Ã£o, com relatÃ³rios periÃ³dicos que avaliem o impacto das soluÃ§Ãµes no julgamento justo, imparcial e eficiente.</h4>
<h4>Â§ 2Âº Verificado viÃ©s discriminatÃ³rio ou incompatibilidade da soluÃ§Ã£o de inteligÃªncia artificial com os princÃ­pios previstos nesta ResoluÃ§Ã£o, deverÃ£o ser adotadas as medidas corretivas necessÃ¡rias, incluindo a suspensÃ£o temporÃ¡ria (imediata ou programada), a correÃ§Ã£o ou, se necessÃ¡rio, a eliminaÃ§Ã£o definitiva da soluÃ§Ã£o ou de seu viÃ©s.</h4>
<h4>Â§ 3Âº Caso se constate a impossibilidade de eliminaÃ§Ã£o do viÃ©s discriminatÃ³rio, a soluÃ§Ã£o de inteligÃªncia artificial deverÃ¡ ser descontinuada, com o consequente cancelamento do registro de seu projeto no Sinapses, e relatÃ³rio das medidas adotadas e das razÃµes que justificaram a decisÃ£o, que poderÃ¡ ser submetido Ã  anÃ¡lise independente para realizaÃ§Ã£o de estudos, se for o caso.</h4>
<h2>CAPÃTULO III</h2>
<h3>DA CATEGORIZAÃ‡ÃƒO DOS RISCOS</h3>
<h3>Art. 9Âº Os tribunais deverÃ£o realizar a avaliaÃ§Ã£o das soluÃ§Ãµes que utilizem tÃ©cnicas de inteligÃªncia artificial, com a finalidade de definir o seu grau de risco, baseando-se na categorizaÃ§Ã£o e nos critÃ©rios previstos neste CapÃ­tulo e no Anexo de ClassificaÃ§Ã£o de Riscos, com base em fatores como o potencial impacto nos direitos fundamentais, a complexidade do modelo, a sustentabilidade financeira, os usos pretendidos e potenciais e a quantidade de dados sensÃ­veis utilizados.</h3>
<h4>Â§ 1Âº A avaliaÃ§Ã£o deverÃ¡ ser realizada pelo tribunal desenvolvedor ou contratante da soluÃ§Ã£o, preferencialmente durante o perÃ­odo de testes e homologaÃ§Ã£o ou, no caso de aplicaÃ§Ãµes de baixo risco, no inÃ­cio da entrada em produÃ§Ã£o interna da soluÃ§Ã£o de IA, de acordo com diretrizes claras e critÃ©rios objetivos que garantam uniformidade na avaliaÃ§Ã£o de risco, que serÃ£o publicadas na plataforma Sinapses, previamente Ã  disponibilizaÃ§Ã£o da soluÃ§Ã£o na PDPJ-Br.</h4>
<h4>Â§ 2Âº O ComitÃª Nacional de InteligÃªncia Artificial do JudiciÃ¡rio fixarÃ¡ as diretrizes e os critÃ©rios de avaliaÃ§Ã£o de risco a que se refere o Â§ 1Âº, ouvidos os tribunais, desenvolvedores e a sociedade civil.</h4>
<h4>Â§ 3Âº O ComitÃª Nacional de InteligÃªncia Artificial do JudiciÃ¡rio poderÃ¡, de ofÃ­cio ou mediante provocaÃ§Ã£o fundamentada, determinar a reclassificaÃ§Ã£o do grau de risco de determinada soluÃ§Ã£o, bem como determinar, de forma justificada, a realizaÃ§Ã£o de avaliaÃ§Ã£o de impacto algorÃ­tmico, quando tal medida se demonstrar proporcional, respeitada tanto quanto possÃ­vel a autonomia dos tribunais.</h4>
<h3>Art. 10. SÃ£o vedados ao Poder JudiciÃ¡rio, por acarretarem risco excessivo Ã  seguranÃ§a da informaÃ§Ã£o, aos direitos fundamentais dos cidadÃ£os ou Ã  independÃªncia dos magistrados, o desenvolvimento e a utilizaÃ§Ã£o de soluÃ§Ãµes:</h3>
<h5>I â€“ que nÃ£o possibilitem a revisÃ£o humana dos resultados propostos ao longo de seu ciclo de treinamento, desenvolvimento e uso, ou que gerem dependÃªncia absoluta do usuÃ¡rio em relaÃ§Ã£o ao resultado proposto, sem possibilidade de alteraÃ§Ã£o ou revisÃ£o;</h5>
<h5>II â€“ que valorem traÃ§os da personalidade, caracterÃ­sticas ou comportamentos de pessoas naturais ou de grupos de pessoas naturais, para fins de avaliar ou prever o cometimento de crimes ou a probabilidade de reiteraÃ§Ã£o delitiva na fundamentaÃ§Ã£o de decisÃµes judiciais, bem como para fins preditivos ou estatÃ­sticos com o propÃ³sito de fundamentar decisÃµes em matÃ©ria trabalhista a partir da formulaÃ§Ã£o de perfis</h5>
<p>pessoais;</p>
<h5>III â€“ que classifiquem ou ranqueiem pessoas naturais, com base no seu comportamento ou situaÃ§Ã£o social ou ainda em atributos da sua personalidade, para a avaliaÃ§Ã£o da plausibilidade de seus direitos, mÃ©ritos judiciais ou testemunhos; e</h5>
<h5>IV â€“ a identificaÃ§Ã£o e a autenticaÃ§Ã£o de padrÃµes biomÃ©tricos para o reconhecimento de emoÃ§Ãµes.</h5>
<h4>Â§ 1Âº Os tribunais deverÃ£o implementar mecanismos de monitoramento contÃ­nuo para garantir o cumprimento dessas vedaÃ§Ãµes e monitorar o desenvolvimento de soluÃ§Ãµes de IA a fim de prevenir o uso inadvertido das tecnologias proibidas.</h4>
<h4>Â§ 2Âº Qualquer soluÃ§Ã£o de IA que, ao longo de seu uso, se enquadrar nas vedaÃ§Ãµes deste artigo, deverÃ¡ ser descontinuada, com registro no Sinapses das razÃµes e providÃªncias adotadas, para anÃ¡lise pelo ComitÃª Nacional de InteligÃªncia Artificial do JudiciÃ¡rio, com fins de buscar prevenir outros casos.</h4>
<h3>Art. 11. Consideram-se de alto ou baixo risco, conforme o caso, as soluÃ§Ãµes que utilizem tÃ©cnicas de inteligÃªncia artificial, desenvolvidas e utilizadas para as finalidades e contextos descritos no Anexo de ClassificaÃ§Ã£o de Riscos desta ResoluÃ§Ã£o.</h3>
<h4>Â§ 1Âº As soluÃ§Ãµes de alto risco deverÃ£o ser submetidas a processos regulares de auditoria e monitoramento contÃ­nuo para supervisionar seu uso e mitigar potenciais riscos aos direitos fundamentais, Ã  privacidade e Ã  justiÃ§a.</h4>
<h4>Â§ 2Âº A categorizaÃ§Ã£o disposta no Anexo de ClassificaÃ§Ã£o de Riscos para soluÃ§Ãµes de alto risco serÃ¡ revista pelo menos anualmente, pelo ComitÃª Nacional de InteligÃªncia Artificial do JudiciÃ¡rio, na forma do inciso I do art. 16 desta ResoluÃ§Ã£o, para assegurar que a classificaÃ§Ã£o de contextos de alto risco permaneÃ§a atualizada e continue adequada Ã s exigÃªncias legais e Ã©ticas.</h4>
<h4>Â§ 3Âº As soluÃ§Ãµes de baixo risco deverÃ£o ser monitoradas e revisadas periodicamente, para assegurar que permaneÃ§am dentro dos parÃ¢metros de baixo risco e que eventuais mudanÃ§as tecnolÃ³gicas ou contextuais nÃ£o alterem essa categorizaÃ§Ã£o.</h4>
<h2>CAPÃTULO IV</h2>
<h3>DAS MEDIDAS DE GOVERNANÃ‡A</h3>
<h3>Art. 12. O tribunal desenvolvedor ou contratante deverÃ¡ estabelecer processos internos aptos a garantir a seguranÃ§a dos sistemas de inteligÃªncia artificial, incluindo, ao menos:</h3>
<h5>I â€“ medidas de transparÃªncia quanto ao emprego e Ã  governanÃ§a dos sistemas de IA, com a publicaÃ§Ã£o de relatÃ³rios que detalhem o funcionamento dos sistemas, suas finalidades, dados utilizados e mecanismos de supervisÃ£o;</h5>
<h5>II â€“ a prevenÃ§Ã£o e mitigaÃ§Ã£o de potenciais vieses discriminatÃ³rios ilegais ou abusivos, por meio de monitoramento contÃ­nuo, com a anÃ¡lise de resultados e a correÃ§Ã£o de eventuais desvios, garantindo a revisÃ£o periÃ³dica dos modelos de IA;</h5>
<h5>III â€“ a implementaÃ§Ã£o de mecanismos de governanÃ§a que garantam o acompanhamento contÃ­nuo dos sistemas de IA, prevendo a definiÃ§Ã£o de pessoas ou comitÃªs internos responsÃ¡veis pela fiscalizaÃ§Ã£o do cumprimento das diretrizes de seguranÃ§a e transparÃªncia, bem como pela anÃ¡lise de relatÃ³rios e recomendaÃ§Ãµes de melhorias;</h5>
<h5>IV â€“ a diretriz para que seja priorizado o desenvolvimento de soluÃ§Ãµes interoperÃ¡veis, que possam ser compartilhadas e integradas entre diferentes Ã³rgÃ£os</h5>
<p>judiciais, evitando a duplicaÃ§Ã£o de esforÃ§os e garantindo eficiÃªncia no uso de recursos tecnolÃ³gicos;</p>
<h5>V â€“ a determinaÃ§Ã£o de que sÃ³ deverÃ£o ser adotadas soluÃ§Ãµes de cÃ³digo aberto ou comerciais que permitam flexibilidade de adaptaÃ§Ã£o aos contextos locais, desde que respeitadas as diretrizes de seguranÃ§a, transparÃªncia e proteÃ§Ã£o de dados pessoais;</h5>
<h5>VI â€“ a orientaÃ§Ã£o de que as soluÃ§Ãµes de IA devem ser tratadas com prÃ¡ticas de gestÃ£o de produto, que incluam fases de definiÃ§Ã£o de requisitos, desenvolvimento, testes, implementaÃ§Ã£o, suporte e melhorias contÃ­nuas, com revisÃµes que garantam a evoluÃ§Ã£o dessas soluÃ§Ãµes e a mitigaÃ§Ã£o de riscos associados;</h5>
<h5>VII â€“ a diretriz de incentivo ao desenvolvimento de interfaces de programaÃ§Ã£o de aplicaÃ§Ãµes (APIs) que permitam a interoperabilidade para comunicaÃ§Ã£o direta com os sistemas tecnolÃ³gicos de outras instituiÃ§Ãµes pÃºblicas que atuam junto Ã  estrutura de JustiÃ§a, garantindo-se a celeridade, seguranÃ§a e integridade dos dados; e</h5>
<h5>VIII â€“ acesso Ã  OAB, Ã  advocacia pÃºblica, ao MinistÃ©rio PÃºblico e Ã s Defensorias, conforme o caso, aos relatÃ³rios de auditoria e monitoramento e Ã  parametrizaÃ§Ã£o ao longo do ciclo de vida da soluÃ§Ã£o que envolver o uso de inteligÃªncia artificial, nos termos desta ResoluÃ§Ã£o.</h5>
<h3>Art. 13. Antes de ser colocada em produÃ§Ã£o, a soluÃ§Ã£o que utilize modelos de inteligÃªncia artificial de alto risco deverÃ¡ adotar as seguintes medidas de governanÃ§a:</h3>
<h5>I â€“ sempre que tecnicamente possÃ­vel, utilizar dados de treinamento, validaÃ§Ã£o e teste que sejam adequados, representativos e equilibrados, contendo propriedades estatÃ­sticas apropriadas em relaÃ§Ã£o Ã s pessoas afetadas e levando em conta caracterÃ­sticas e elementos especÃ­ficos do contexto geogrÃ¡fico, comportamental ou funcional no qual o sistema de IA de alto risco serÃ¡ utilizado;</h5>
<h5>II â€“ registro de fontes automatizadas e do grau de supervisÃ£o humana que tenham contribuÃ­do para os resultados apresentados pelos sistemas IA, a serem submetidos a auditorias regulares e monitoramento contÃ­nuo;</h5>
<h5>III â€“ indicaÃ§Ã£o clara e em linguagem simples dos objetivos e resultados pretendidos pelo uso do modelo de IA, de forma que possam ser compreendidos pelos usuÃ¡rios e supervisionados pelos magistrados;</h5>
<h5>IV â€“ documentaÃ§Ã£o em linguagem simples, no formato adequado a cada agente de IA e Ã  tecnologia usada, do funcionamento do sistema e das decisÃµes envolvidas em sua construÃ§Ã£o, considerando todas as etapas relevantes no ciclo de vida do sistema e atualizado sempre que o sistema evolua;</h5>
<h5>V â€“ uso de ferramentas ou processos de registro automÃ¡tico da operaÃ§Ã£o do sistema (log), sempre que tecnicamente possÃ­vel, para permitir a avaliaÃ§Ã£o periÃ³dica de sua acurÃ¡cia e robustez, apurar potenciais resultados discriminatÃ³rios, com implementaÃ§Ã£o das medidas de mitigaÃ§Ã£o de riscos e atenÃ§Ã£o para efeitos adversos e identificar eventual uso malicioso ou indevido do sistema;</h5>
<h5>VI â€“ medidas para mitigar e prevenir vieses discriminatÃ³rios, bem como polÃ­ticas de gestÃ£o e governanÃ§a para promoÃ§Ã£o da responsabilidade social e sustentÃ¡vel; e</h5>
<h5>VII â€“ adoÃ§Ã£o de medidas para viabilizar a explicabilidade adequada, sempre que tecnicamente possÃ­vel, dos resultados dos sistemas de IA e de medidas para disponibilizar informaÃ§Ãµes adequadas em linguagem simples e acessÃ­vel que permitam a interpretaÃ§Ã£o dos seus resultados e funcionamento, respeitados o direito de autor, a propriedade intelectual e os sigilos industrial e comercial, mas garantida a transparÃªncia mÃ­nima necessÃ¡ria para atender ao disposto nesta ResoluÃ§Ã£o.</h5>
<h3>Art. 14. O tribunal desenvolvedor ou contratante deverÃ¡ promover avaliaÃ§Ã£o de impacto algorÃ­tmico da soluÃ§Ã£o classificada na avaliaÃ§Ã£o como de alto risco, nos termos do art. 11 desta ResoluÃ§Ã£o.</h3>
<h4>Â§ 1Âº A avaliaÃ§Ã£o de impacto algorÃ­tmico consistirÃ¡ em processo contÃ­nuo e executado conforme as diretrizes tÃ©cnicas e os requisitos formulados previamente pelo ComitÃª Nacional de InteligÃªncia Artificial do JudiciÃ¡rio, incluindo auditorias regulares, monitoramento contÃ­nuo, revisÃµes periÃ³dicas e a adoÃ§Ã£o de aÃ§Ãµes corretivas quando necessÃ¡rio.</h4>
<h4>Â§ 2Âº A elaboraÃ§Ã£o da avaliaÃ§Ã£o de impacto deve, sempre que possÃ­vel, incluir a participaÃ§Ã£o pÃºblica, ainda que de maneira simplificada, e o acompanhamento, com acesso aos relatÃ³rios, de representante da OAB, do MinistÃ©rio PÃºblico e da Defensoria PÃºblica.</h4>
<h4>Â§ 3Âº As conclusÃµes da avaliaÃ§Ã£o de impacto, incluindo eventuais aÃ§Ãµes corretivas adotadas, serÃ£o pÃºblicas e disponibilizadas na plataforma Sinapses, por meio de relatÃ³rios claros e acessÃ­veis, de forma a permitir o entendimento por magistrados, servidores e o pÃºblico em geral.</h4>
<h2>CAPÃTULO V</h2>
<h3>DA SUPERVISÃƒO E IMPLEMENTAÃ‡ÃƒO</h3>
<h3>Art. 15. Fica instituÃ­do o ComitÃª Nacional de InteligÃªncia Artificial do JudiciÃ¡rio.</h3>
<h4>Â§ 1Âº O ComitÃª serÃ¡ formado por 14 (quatorze) membros titulares e 13 (treze) suplentes, divididos por categoria e designados por ato do Presidente do CNJ, a partir das seguintes origens:</h4>
<h5>I â€“ dois Conselheiros do CNJ, ambos titulares, sendo ao menos um deles membro da ComissÃ£o Permanente de Tecnologia da InformaÃ§Ã£o;</h5>
<h5>II â€“ dois juÃ­zes auxiliares e dois servidores, ambos do CNJ, com experiÃªncia na Ã¡rea;</h5>
<h5>III â€“ dois magistrados, sendo um representante do Conselho da JustiÃ§a Federal (CJF) e um representante do Conselho Superior da JustiÃ§a do Trabalho (CSJT);</h5>
<h5>IV â€“ quatro desembargadores, sendo um representante de tribunal de justiÃ§a, um representante de tribunal regional federal, um representante de tribunal regional do trabalho e um representante de tribunal eleitoral;</h5>
<h5>V â€“ dois representantes das escolas da magistratura, sendo um da Escola Nacional de FormaÃ§Ã£o e AperfeiÃ§oamento de Magistrados (Enfam) e um da Escola Nacional de FormaÃ§Ã£o e AperfeiÃ§oamento de Magistrados do Trabalho (Enamat);</h5>
<h5>VI â€“ quatro magistrados, escolhidos a partir de indicaÃ§Ãµes da AssociaÃ§Ã£o dos Magistrados Brasileiros (AMB), AssociaÃ§Ã£o Nacional dos Magistrados da JustiÃ§a do Trabalho (Anamatra) e AssociaÃ§Ã£o dos JuÃ­zes Federais do Brasil (Ajufe);</h5>
<h5>VII â€“ dois representantes da OAB;</h5>
<h5>VIII â€“ dois representantes do MinistÃ©rio PÃºblico;</h5>
<h5>IX â€“ dois representantes da Defensoria PÃºblica; e</h5>
<h5>X â€“ dois representantes da sociedade civil, preferencialmente com notÃ³rio saber ou sÃ³lida atuaÃ§Ã£o profissional nas Ã¡reas de inteligÃªncia artificial, tecnologia da informaÃ§Ã£o, governanÃ§a de inteligÃªncia artificial e direitos humanos.</h5>
<h4>Â§ 2Âº A presidÃªncia do ComitÃª, que terÃ¡ voto de qualidade, caberÃ¡ ao Conselheiro eleito pelo PlenÃ¡rio do CNJ, cabendo ao outro Conselheiro a vice-presidÃªncia.</h4>
<h4>Â§ 3Âº Os membros referidos nos incisos I a VI terÃ£o voz e voto, enquanto os membros referidos nos incisos VII em diante terÃ£o direito apenas a voz no Ã¢mbito do ComitÃª.</h4>
<h4>Â§ 4Âº Em casos de comprovada urgÃªncia, poderÃ£o ser exaradas medidas pelo Presidente do ComitÃª Nacional de InteligÃªncia Artificial do JudiciÃ¡rio ad referendum da composiÃ§Ã£o plena do ComitÃª.</h4>
<h4>Â§ 5Âº As decisÃµes, manifestaÃ§Ãµes ou processos do ComitÃª Nacional de InteligÃªncia Artificial do JudiciÃ¡rio poderÃ£o ser submetidos ao PlenÃ¡rio do CNJ, de ofÃ­cio ou mediante provocaÃ§Ã£o, nos termos do art. 98 de seu Regimento Interno, que, no exercÃ­cio de sua competÃªncia originÃ¡ria, poderÃ¡ decidir, ratificar, reformar, avocar ou arquivar atos, processos ou expedientes relativos Ã s competÃªncias atribuÃ­das ao ComitÃª nesta ResoluÃ§Ã£o.</h4>
<h4>Â§ 6Âº Para a designaÃ§Ã£o prevista no Â§ 1Âº, o Presidente do CNJ poderÃ¡ solicitar a indicaÃ§Ã£o de nomes a autoridades ou entidades representativas, cabendo-lhe a designaÃ§Ã£o final como titular ou suplente em cada categoria, bem como sua substituiÃ§Ã£o, quando for o caso, para contemplar outro representante da mesma categoria.</h4>
<h3>Art. 16. Compete ao ComitÃª Nacional de InteligÃªncia Artificial do JudiciÃ¡rio:</h3>
<h5>I â€“ avaliar a necessidade de atualizaÃ§Ã£o das hipÃ³teses de categorizaÃ§Ã£o de riscos referidas no art. 11 e dispostas no Anexo de ClassificaÃ§Ã£o de Riscos desta ResoluÃ§Ã£o, com base em critÃ©rios objetivos e conforme as melhores prÃ¡ticas internacionais;</h5>
<h5>II â€“ reclassificar determinados sistemas contratados ou desenvolvidos pelos tribunais, nos termos do Â§ 3Âº do art. 9Âº desta ResoluÃ§Ã£o, com a devida justificativa e a publicaÃ§Ã£o de relatÃ³rio tÃ©cnico de reclassificaÃ§Ã£o, de ofÃ­cio ou mediante provocaÃ§Ã£o.</h5>
<h5>III â€“ estabelecer normas e diretrizes negociais para o sistema Sinapses, incluindo normas de governanÃ§a, transparÃªncia, auditoria e monitoramento;</h5>
<h5>IV â€“ consolidar padrÃµes de governanÃ§a e mapeamento de riscos conhecidos e nÃ£o conhecidos que permitam o cumprimento desta ResoluÃ§Ã£o, a definiÃ§Ã£o e a reavaliaÃ§Ã£o contÃ­nua do grau de risco adequado para cada hipÃ³tese de aplicaÃ§Ã£o, ouvidos os tribunais, especialistas externos e a sociedade civil;</h5>
<h5>V â€“ sugerir que o CNJ celebre e realize convÃªnios e acordos de cooperaÃ§Ã£o com outros Ã³rgÃ£os nacionais e internacionais, visando Ã  melhoria contÃ­nua dos sistemas de IA e Ã  incorporaÃ§Ã£o das melhores prÃ¡ticas globais;</h5>
<h5>VI â€“ avaliar a conveniÃªncia do uso, de ofÃ­cio ou mediante provocaÃ§Ã£o, de soluÃ§Ãµes de IA disponÃ­veis no mercado, gratuitas ou nÃ£o, que poderÃ£o ser utilizadas pelos magistrados e servidores do Poder JudiciÃ¡rio no exercÃ­cio das funÃ§Ãµes do seu cargo no JudiciÃ¡rio, por meio de licenÃ§a privada, considerando em particular as condiÃ§Ãµes de uso dos dados pessoais e dos dados para treinamento, os critÃ©rios de seguranÃ§a e o grau de risco das aplicaÃ§Ãµes, estabelecendo regras adicionais de governanÃ§a e monitoramento, caso necessÃ¡rio, nos termos desta ResoluÃ§Ã£o;</h5>
<h5>VII â€“ monitorar a oferta pelos tribunais de capacitaÃ§Ã£o e treinamento em inteligÃªncia artificial aos seus magistrados e servidores, bem como solicitar ou sugerir Ã  Enfam e Ã  Enamat que desenvolvam parÃ¢metros curriculares e aÃ§Ãµes voltadas Ã  capacitaÃ§Ã£o e ao treinamento em inteligÃªncia artificial;</h5>
<h5>VIII â€“ determinar a realizaÃ§Ã£o ou estabelecer a periodicidade mÃ­nima para que sejam realizadas auditorias e aÃ§Ãµes de monitoramento das soluÃ§Ãµes de inteligÃªncia artificial, alÃ©m de disciplinar os prazos para a confecÃ§Ã£o dos relatÃ³rios e para o cadastramento na plataforma Sinapses;</h5>
<h5>IX â€“ definir e implementar protocolos tÃ©cnicos padronizados de auditoria, garantindo que todos os sistemas de IA utilizados pelo JudiciÃ¡rio sejam auditados antes da implementaÃ§Ã£o e periodicamente, sempre que possÃ­vel; e</h5>
<h5>X â€“ estabelecer padrÃµes de transparÃªncia, incluindo a exigÃªncia de documentaÃ§Ã£o detalhada e publicaÃ§Ã£o de relatÃ³rios regulares de impacto e desempenho, respeitado o estado-da-arte da tecnologia e o disposto nesta ResoluÃ§Ã£o.</h5>
<h4>Â§ 1Âº A avaliaÃ§Ã£o periÃ³dica de que trata o inciso I deste artigo, que poderÃ¡ ser feita no relatÃ³rio previsto no art. 18 desta ResoluÃ§Ã£o e publicada, deverÃ¡ contemplar, alÃ©m de outros pontos que se mostrem relevantes para a administraÃ§Ã£o da justiÃ§a, para a razoÃ¡vel duraÃ§Ã£o do processo e para a garantia de direitos fundamentais:</h4>
<h5>I â€“ a anÃ¡lise geral das soluÃ§Ãµes cadastradas no Sinapses e das soluÃ§Ãµes descontinuadas, descartadas ou vedadas no ano corrente, com a publicaÃ§Ã£o de relatÃ³rios que poderÃ£o trazer conclusÃµes e recomendaÃ§Ãµes;</h5>
<h5>II â€“ a necessÃ¡ria harmonizaÃ§Ã£o com a legislaÃ§Ã£o e com os atos normativos do CNJ, em especial as normas relativas Ã  proteÃ§Ã£o de dados e ao uso da inteligÃªncia artificial;</h5>
<h5>III â€“ a anÃ¡lise das novas tecnologias e inovaÃ§Ãµes que possam influenciar a eficÃ¡cia e a adequaÃ§Ã£o das normas existentes, com a inclusÃ£o de recomendaÃ§Ãµes para ajustes normativos;</h5>
<h5>IV â€“ a verificaÃ§Ã£o de situaÃ§Ãµes em que as regras vigentes se mostrarem insuficientes para o controle dos riscos associados ao uso de inteligÃªncia artificial no Ã¢mbito do Poder JudiciÃ¡rio, com encaminhamentos para correÃ§Ã£o das lacunas identificadas.</h5>
<h4>Â§ 2Âº A vedaÃ§Ã£o ou limitaÃ§Ã£o para o uso de soluÃ§Ãµes baseadas em modelos de linguagem de larga escala (LLMs) e outros sistemas de inteligÃªncia artificial generativa (IAGen) a que se refere o inciso VI do caput deste artigo terÃ¡ como critÃ©rio eventual descumprimento ou fundado receio de risco de descumprimento das diretrizes dispostas no Â§ 3Âº do art. 19 desta ResoluÃ§Ã£o, e poderÃ¡ limitar o uso de determinada ferramenta apenas a soluÃ§Ãµes de baixo risco ou determinar providÃªncias relativas ao uso de dados, assegurada a possibilidade de rever eventual decisÃ£o previamente tomada, se as condiÃ§Ãµes ou os termos de uso da soluÃ§Ã£o forem modificados.</h4>
<h4>Â§ 3Âº Empresas nacionais ou estrangeiras que prestem serviÃ§os de armazenamento, processamento, intermediaÃ§Ã£o digital ou inteligÃªncia artificial ao Poder JudiciÃ¡rio, ou que operem plataformas com impacto direto no exercÃ­cio da jurisdiÃ§Ã£o brasileira, devem observar integralmente as decisÃµes judiciais proferidas no Brasil e atuar em conformidade com a legislaÃ§Ã£o nacional, observando-se o seguinte:</h4>
<h6>a) os tribunais deverÃ£o adotar mecanismos de monitoramento contÃ­nuo para identificar eventuais descumprimentos de decisÃµes judiciais por parte dessas empresas, comunicando tais infraÃ§Ãµes Ã s autoridades competentes para adoÃ§Ã£o das medidas cabÃ­veis;</h6>
<h6>b) nos contratos firmados com empresas de tecnologia, deverÃ£o ser incluÃ­das clÃ¡usulas contratuais que exijam o cumprimento da legislaÃ§Ã£o e das decisÃµes judiciais brasileiras, prevendo expressamente a possibilidade de rescisÃ£o contratual e a aplicaÃ§Ã£o das penalidades em caso de descumprimento.</h6>
<h3>Art. 17. Para embasar a avaliaÃ§Ã£o de atualizaÃ§Ã£o das hipÃ³teses de categorizaÃ§Ã£o de riscos, o ComitÃª Nacional de InteligÃªncia Artificial do JudiciÃ¡rio considerarÃ¡ as diretrizes dispostas nesta ResoluÃ§Ã£o, alÃ©m dos seguintes critÃ©rios:</h3>
<h5>I â€“ impacto negativo comprovado no exercÃ­cio de direitos e liberdades fundamentais ou na utilizaÃ§Ã£o de serviÃ§os essenciais;</h5>
<h5>II â€“ alto potencial danoso de ordem material ou moral, devidamente mensurado, incluindo discriminaÃ§Ã£o ilegal ou abusiva, direta ou indireta;</h5>
<h5>III â€“ repercussÃ£o significativa sobre pessoas pertencentes a grupos vulnerÃ¡veis, levando em conta suas condiÃ§Ãµes sociais, econÃ´micas e culturais;</h5>
<h5>IV â€“ irreversibilidade ou difÃ­cil reversÃ£o de possÃ­veis resultados prejudiciais da soluÃ§Ã£o, especialmente em casos que afetem diretamente direitos materiais ou processuais, ou que provoquem movimentaÃ§Ã£o automÃ¡tica relevante em processos judiciais;</h5>
<h5>V â€“ histÃ³rico de responsabilizaÃ§Ã£o civil ou administrativa em decorrÃªncia da potencial violaÃ§Ã£o a direitos morais ou materiais dos usuÃ¡rios externos pela soluÃ§Ã£o de inteligÃªncia artificial, devidamente documentado e analisado em relatÃ³rios tÃ©cnicos;</h5>
<h5>VI â€“ baixo grau de transparÃªncia, de explicabilidade e de auditabilidade da soluÃ§Ã£o, com critÃ©rios objetivos que dificultem ou impossibilitem seu controle, supervisÃ£o e revisÃ£o pelas partes eventualmente interessadas; e</h5>
<h5>VII â€“ alto nÃ­vel de identificabilidade dos titulares dos dados, especialmente quando o tratamento envolve combinaÃ§Ã£o, correspondÃªncia ou comparaÃ§Ã£o de dados de vÃ¡rias fontes, com impacto direto na privacidade e na proteÃ§Ã£o dos dados pessoais.</h5>
<h4>Â§ 1Âº A avaliaÃ§Ã£o de risco deverÃ¡ ser acompanhada de indicadores de desempenho e relatÃ³rios de auditoria ou de monitoramento, a fim de garantir a efetividade das medidas de mitigaÃ§Ã£o de riscos.</h4>
<h4>Â§ 2Âº Constatada a baixa transparÃªncia ou explicabilidade de uma soluÃ§Ã£o de IA, medidas corretivas deverÃ£o ser adotadas sempre que tecnicamente possÃ­vel, incluindo eventual descontinuidade da soluÃ§Ã£o, caso as correÃ§Ãµes nÃ£o sejam viÃ¡veis.</h4>
<h3>Art. 18. O ComitÃª Nacional de InteligÃªncia Artificial do JudiciÃ¡rio confeccionarÃ¡ relatÃ³rio circunstanciado de sua avaliaÃ§Ã£o anual, contendo:</h3>
<h5>I â€“ as metodologias e critÃ©rios utilizados na avaliaÃ§Ã£o das soluÃ§Ãµes de inteligÃªncia artificial;</h5>
<h5>II â€“ os resultados das auditorias, monitoramentos e avaliaÃ§Ãµes de impacto algorÃ­tmico realizadas;</h5>
<h5>III â€“ a atualizaÃ§Ã£o das hipÃ³teses de categorizaÃ§Ã£o de riscos dispostas no Anexo de ClassificaÃ§Ã£o de Riscos desta ResoluÃ§Ã£o, quando for o caso;</h5>
<h5>IV â€“ recomendaÃ§Ãµes para a correÃ§Ã£o de falhas ou a melhoria das soluÃ§Ãµes de inteligÃªncia artificial em uso, conforme identificado nas auditorias, monitoramentos ou avaliaÃ§Ãµes; e</h5>
<h5>V â€“ panorama do estado da utilizaÃ§Ã£o da inteligÃªncia artificial, generativa ou nÃ£o, no JudiciÃ¡rio brasileiro.</h5>
<h4>Â§ 1Âº O relatÃ³rio serÃ¡ publicado e disponibilizado ao pÃºblico em geral, garantindo a transparÃªncia do processo de avaliaÃ§Ã£o e acompanhamento das soluÃ§Ãµes de IA utilizadas no JudiciÃ¡rio.</h4>
<h4>Â§ 2Âº O ComitÃª poderÃ¡ propor revisÃµes extraordinÃ¡rias a qualquer momento, caso sejam identificadas mudanÃ§as tecnolÃ³gicas significativas ou novas informaÃ§Ãµes que justifiquem uma reavaliaÃ§Ã£o dos riscos associados Ã s soluÃ§Ãµes de IA em uso.</h4>
<h4>Â§ 3Âº Os documentos confeccionados com base nesta ResoluÃ§Ã£o deverÃ£o ser disponibilizados em formatos acessÃ­veis, garantindo a inclusÃ£o de pessoas com deficiÃªncia e outros grupos vulnerÃ¡veis, garantindo ampla transparÃªncia.</h4>
<h2>CAPÃTULO VI</h2>
<h3>DO USO E DA CONTRATAÃ‡ÃƒO DE MODELOS DE LINGUAGEM DE</h3>
<p>LARGA ESCALA (LLMs) E DE OUTROS SISTEMAS DE IA GENERATIVA (IAGen)</p>
<h3>Art. 19. Os modelos de linguagem de larga escala (LLMs), de pequena escala (SLMS) e outros sistemas de inteligÃªncia artificial generativa (IAGen) disponÃ­veis na rede mundial de computadores poderÃ£o ser utilizados pelos magistrados e pelos servidores do Poder JudiciÃ¡rio em suas respectivas atividades como ferramentas de auxÃ­lio Ã  gestÃ£o ou de apoio Ã  decisÃ£o, em obediÃªncia aos padrÃµes de seguranÃ§a da informaÃ§Ã£o e Ã s normas desta ResoluÃ§Ã£o.</h3>
<h4>Â§ 1Âº Os modelos e soluÃ§Ãµes a que se refere o caput poderÃ£o ser utilizados pelos magistrados e pelos servidores do Poder JudiciÃ¡rio, preferencialmente, por meio de acesso que seja habilitado, disponibilizado e monitorado pelos tribunais.</h4>
<h4>Â§ 2Âº Quando o tribunal nÃ£o oferecer soluÃ§Ã£o corporativa de inteligÃªncia artificial especificamente treinada e personalizada para uso no Poder JudiciÃ¡rio, serÃ¡ facultado ao magistrado, servidor ou colaborador do Poder JudiciÃ¡rio a contrataÃ§Ã£o direta de soluÃ§Ã£o mediante assinatura ou cadastro de natureza privada, desde que atendidas as diretrizes do Â§ 3Âº deste artigo.</h4>
<h4>Â§ 3Âº A contrataÃ§Ã£o direta para uso privado ou individual dos modelos de linguagem de larga escala (LLMs) e outros sistemas de inteligÃªncia artificial generativa (IAGen) disponÃ­veis na rede mundial de computadores, para fins de uso em atividades funcionais do Poder JudiciÃ¡rio deverÃ¡ observar as seguintes condiÃ§Ãµes:</h4>
<h5>I â€“ os usuÃ¡rios deverÃ£o realizar capacitaÃ§Ã£o e treinamentos especÃ­ficos sobre melhores prÃ¡ticas, limitaÃ§Ãµes, riscos, e uso Ã©tico, responsÃ¡vel e eficiente de LLMs e dos sistemas de IA generativa para a utilizaÃ§Ã£o em suas atividades, conforme programa de letramento digital padronizado, nos termos do inciso VII do art. 16 desta ResoluÃ§Ã£o, ficando a cargo dos tribunais e de suas escolas a promoÃ§Ã£o dos treinamentos continuados aos magistrados e servidores;</h5>
<h5>II â€“ o uso dessas ferramentas serÃ¡ de carÃ¡ter auxiliar e complementar, consistindo em mecanismos de apoio Ã  decisÃ£o, vedada a utilizaÃ§Ã£o como instrumento autÃ´nomo de tomada de decisÃµes judiciais sem a devida orientaÃ§Ã£o, interpretaÃ§Ã£o, verificaÃ§Ã£o e revisÃ£o por parte do magistrado, que permanecerÃ¡ integralmente responsÃ¡vel pelas decisÃµes tomadas e pelas informaÃ§Ãµes nelas contidas;</h5>
<h5>III â€“ as empresas fornecedoras dos serviÃ§os de LLMs e IA generativa devem observar padrÃµes de polÃ­tica de proteÃ§Ã£o de dados e de propriedade intelectual, em conformidade com a legislaÃ§Ã£o aplicÃ¡vel, sendo vedado o tratamento, uso ou compartilhamento dos dados fornecidos pelos usuÃ¡rios do Poder JudiciÃ¡rio, bem como dos dados inferidos a partir desses, para treinamento, aperfeiÃ§oamento ou quaisquer outros fins nÃ£o expressamente autorizados;</h5>
<h5>IV â€“ Ã© vedado o uso de LLMs e sistemas de IA generativa de natureza privada ou externos ao JudiciÃ¡rio para processar, analisar, gerar conteÃºdo ou servir de suporte a decisÃµes a partir de documentos ou dados sigilosos ou protegidos por segredo de justiÃ§a, nos termos da legislaÃ§Ã£o aplicÃ¡vel, salvo quando devidamente anonimizados na origem ou quando forem adotados mecanismos tÃ©cnicos e procedimentais</h5>
<p>que garantam a efetiva proteÃ§Ã£o e seguranÃ§a desses dados e de seus titulares; e</p>
<h5>V â€“ Ã© vedado o uso de LLMs e sistemas de IA generativa de natureza privada ou externos ao JudiciÃ¡rio para as finalidades previstas nesta ResoluÃ§Ã£o como de risco excessivo ou de alto risco, nos termos do art. 10 e 11 desta ResoluÃ§Ã£o.</h5>
<h4>Â§ 4Âº O ComitÃª Nacional de InteligÃªncia Artificial do JudiciÃ¡rio elaborarÃ¡ e atualizarÃ¡ periodicamente um manual de boas prÃ¡ticas em linguagem simples para orientar magistrados e servidores sobre o uso correto, Ã©tico e eficiente de LLMs e de sistemas de IA generativa, abordando aspectos como suas potencialidades, limitaÃ§Ãµes, configuraÃ§Ãµes recomendadas, riscos, casos de uso adequados e vedados, orientaÃ§Ãµes para interpretaÃ§Ã£o crÃ­tica dos resultados e correÃ§Ã£o de eventuais erros ou inconsistÃªncias.</h4>
<h4>Â§ 5Âº CaberÃ¡ aos tribunais e Ã s suas escolas, em consonÃ¢ncia com as diretrizes do CNJ, da Enfam e da Enamat, promover capacitaÃ§Ã£o e treinamentos continuados para assegurar o uso adequado e responsÃ¡vel de LLMs e sistemas de IA generativa pelos magistrados e servidores, bem como para mantÃª-los atualizados quanto Ã  evoluÃ§Ã£o dessas tecnologias e suas implicaÃ§Ãµes para o sistema de JustiÃ§a.</h4>
<h4>Â§ 6Âº Quando houver emprego de IA generativa para auxÃ­lio Ã  redaÃ§Ã£o de ato judicial, tal situaÃ§Ã£o poderÃ¡ ser mencionada no corpo da decisÃ£o, a critÃ©rio do magistrado, sendo, porÃ©m, devido o registro automÃ¡tico no sistema interno do tribunal, para fins de produÃ§Ã£o de estatÃ­sticas, monitoramento e eventual auditoria.</h4>
<h4>Â§ 7Âº Na hipÃ³tese do Â§ 2Âº deste artigo, o magistrado que contratar soluÃ§Ã£o de mercado de inteligÃªncia artificial para uso em suas atividades no Poder JudiciÃ¡rio, ou o gestor que tiver em sua equipe servidor ou colaborador que utilize essas soluÃ§Ãµes, deverÃ¡ prestar informaÃ§Ãµes ao seu respectivo tribunal sobre sua utilizaÃ§Ã£o, na forma do regulamento.</h4>
<h4>Â§ 8Âº Os tribunais consolidarÃ£o as informaÃ§Ãµes recebidas na forma do Â§ 7Âº deste artigo para envio ao ComitÃª Nacional de InteligÃªncia Artificial do JudiciÃ¡rio, que as utilizarÃ¡ para os fins previstos no art. 25 desta ResoluÃ§Ã£o.</h4>
<h3>Art. 20. A contrataÃ§Ã£o de modelos de linguagem de larga escala (LLMs), de pequena escala (SLMS) e outros sistemas de inteligÃªncia artificial generativa (IAGen) pelos tribunais deverÃ¡ cumprir as seguintes diretrizes:</h3>
<h5>I â€“ a empresa contratada deve se comprometer a respeitar a legislaÃ§Ã£o vigente no Brasil, entre elas, a Lei Complementar nÂº 35/1979 (Lei OrgÃ¢nica da Magistratura Nacional â€“ Loman) , a Lei Geral de ProteÃ§Ã£o de Dados Pessoais, a Lei nÂº 9.279/1996 (Lei de Propriedade Intelectual â€“ LPI) e esta ResoluÃ§Ã£o;</h5>
<h5>II â€“ o uso dos dados fornecidos pelos usuÃ¡rios do Poder JudiciÃ¡rio para treinamento fica condicionado Ã s bases legais da Lei Geral de ProteÃ§Ã£o de Dados Pessoais e nÃ£o poderÃ¡ ser utilizado para quaisquer outros fins nÃ£o expressamente autorizados, com realizaÃ§Ã£o de monitoramento contÃ­nuo para assegurar a conformidade com as diretrizes de proteÃ§Ã£o de dados e de propriedade intelectual;</h5>
<h5>III â€“ Ã© dever dos tribunais contratantes e de suas escolas, da magistratura e de servidores, oferecer treinamento aos usuÃ¡rios internos de LLMs e de sistemas de inteligÃªncia artificial generativa sobre as limitaÃ§Ãµes, os riscos e o uso Ã©tico, responsÃ¡vel e eficiente dessas soluÃ§Ãµes antes de utilizÃ¡-los em suas atividades;</h5>
<h5>IV â€“ o uso dessas ferramentas serÃ¡ de carÃ¡ter auxiliar e complementar, vedada a utilizaÃ§Ã£o como instrumento autÃ´nomo de tomada de decisÃµes judiciais sem a devida orientaÃ§Ã£o, interpretaÃ§Ã£o, verificaÃ§Ã£o e revisÃ£o por parte do magistrado, que permanecerÃ¡ integralmente responsÃ¡vel pelas decisÃµes tomadas e pelas informaÃ§Ãµes nelas contidas;</h5>
<h5>V â€“ Ã© vedado o uso de LLMs e sistemas de IA generativa para processar, analisar, gerar conteÃºdo ou servir de suporte a decisÃµes a partir de documentos ou dados sigilosos ou protegidos por segredo de justiÃ§a, exceto nas hipÃ³teses do art. 19, Â§ 3Âº, IV, desta ResoluÃ§Ã£o;</h5>
<h5>VI â€“ Ã© vedado o uso de LLMs e sistemas de IA generativa privados ou externos ao JudiciÃ¡rio para as finalidades previstas nesta ResoluÃ§Ã£o como de risco excessivo ou de alto risco, nos termos do art. 10 e 11 desta ResoluÃ§Ã£o;</h5>
<h5>VII â€“ as empresas contratadas devem resguardar o sigilo das informaÃ§Ãµes compartilhadas pelos tribunais contratantes, respeitar e comprovar utilizaÃ§Ã£o de normas de seguranÃ§a atuais e compatÃ­veis com o estado da arte, podendo ser exigida auditoria externa ou relatÃ³rios periÃ³dicos sobre a seguranÃ§a dos dados e sua conformidade;</h5>
<h5>VIII â€“ os sistemas contratados devem oferecer documentaÃ§Ã£o e referÃªncias bibliogrÃ¡ficas atualizadas, sempre que disponÃ­veis, de acordo com o uso do seu resultado;</h5>
<h5>IX â€“ os sistemas contratados deverÃ£o adotar mecanismos de privacyby design (privacidade desde a concepÃ§Ã£o) e privacyby default (privacidade por padrÃ£o), incluindo a possibilidade de nÃ£o-armazenamento ou eliminaÃ§Ã£o do histÃ³rico de perguntas e prompts, podendo ser exigido relatÃ³rio com indicadores claros para avaliar sua implementaÃ§Ã£o e cumprimento; e</h5>
<h5>X â€“ a contrataÃ§Ã£o de serviÃ§os ou soluÃ§Ãµes de inteligÃªncia artificial pelos tribunais deverÃ¡ levar em conta seus aspectos financeiros e orÃ§amentÃ¡rios em todo seu ciclo de vida, notadamente no desenvolvimento, implantaÃ§Ã£o e manutenÃ§Ã£o.</h5>
<h4>ParÃ¡grafo Ãºnico. Ã‰ vedada a utilizaÃ§Ã£o de dados sigilosos ou protegidos por segredo de justiÃ§a para treinamento de modelos de inteligÃªncia artificial, salvo prÃ©via anonimizaÃ§Ã£o dos dados na origem.</h4>
<h3>Art. 21. Os sistemas de processo judicial eletrÃ´nico que utilizem soluÃ§Ãµes de inteligÃªncia artificial deverÃ£o indicar, em sua interface principal, a relaÃ§Ã£o dos modelos em uso, sua versÃ£o e cÃ³digo de registro no Sinapses e a data da Ãºltima atualizaÃ§Ã£o dessas informaÃ§Ãµes.</h3>
<h4>Â§ 1Âº A revisÃ£o e atualizaÃ§Ã£o dessas informaÃ§Ãµes ocorrerÃ£o com periodicidade mÃ­nima de 12 (doze) meses ou sempre que houver alteraÃ§Ã£o significativa nos modelos ou em suas versÃµes.</h4>
<h4>Â§ 2Âº Os produtos elaborados de forma automatizada por soluÃ§Ã£o de inteligÃªncia artificial deverÃ£o registrar a utilizaÃ§Ã£o de IA nos logs de uso do sistema por meio de rÃ³tulos de identificaÃ§Ã£o adequados e compreensÃ­veis, para fins de estatÃ­stica, monitoramento e eventual auditoria.</h4>
<h2>CAPÃTULO VII</h2>
<h3>TRANSPARÃŠNCIA E REGISTRO NO SINAPSES</h3>
<h3>Art. 22. Qualquer modelo de inteligÃªncia artificial que venha a ser adotado pelos Ã³rgÃ£os do Poder JudiciÃ¡rio deverÃ¡ observar as regras de governanÃ§a de dados aplicÃ¡veis aos seus prÃ³prios sistemas computacionais, as ResoluÃ§Ãµes e as RecomendaÃ§Ãµes do CNJ, a Lei Geral de ProteÃ§Ã£o de Dados Pessoais, a Lei de Acesso Ã  InformaÃ§Ã£o, a propriedade intelectual e o segredo de justiÃ§a.</h3>
<h4>Â§ 1Âº A conformidade com essas regras deverÃ¡ ser assegurada contratualmente, garantida por meio de monitoramento contÃ­nuo e eventual auditoria, com foco na proteÃ§Ã£o de dados, na propriedade intelectual e na transparÃªncia dos modelos de IA adotados.</h4>
<h4>Â§ 2Âº O uso dos modelos de inteligÃªncia artificial no Ã¢mbito do JudiciÃ¡rio deverÃ¡ ser acompanhado de relatÃ³rios periÃ³dicos, que comprovem a conformidade com as diretrizes de governanÃ§a de dados, em particular os sensÃ­veis, transparÃªncia e proteÃ§Ã£o Ã  propriedade intelectual.</h4>
<h4>Â§ 3Âº Os modelos de inteligÃªncia artificial adotados deverÃ£o possuir mecanismos de explicabilidade, sempre que tecnicamente possÃ­vel, de modo que suas decisÃµes e operaÃ§Ãµes sejam compreensÃ­veis e auditÃ¡veis pelos operadores judiciais.</h4>
<h3>Art. 23. Os Ã³rgÃ£os do Poder JudiciÃ¡rio envolvidos em projeto de inteligÃªncia artificial deverÃ£o:</h3>
<h5>I â€“ informar ao CNJ, por meio da plataforma Sinapses a conclusÃ£o da pesquisa ou estudo, o inÃ­cio do desenvolvimento e a entrada em produÃ§Ã£o da soluÃ§Ã£o de inteligÃªncia artificial, bem como os respectivos objetivos e os resultados que se pretende alcanÃ§ar;</h5>
<h5>II â€“ promover esforÃ§os para atuaÃ§Ã£o em modelo comunitÃ¡rio, com desestÃ­mulo ao desenvolvimento paralelo por um tribunal quando a iniciativa possuir objetivos e resultados pretendidos idÃªnticos e compatÃ­veis com modelo ou sistema de inteligÃªncia artificial jÃ¡ existente em outro tribunal; e</h5>
<h5>III â€“ o depÃ³sito do cÃ³digo-fonte, bases de dados e demais partes da soluÃ§Ã£o de IA poderÃ£o ser dispensados, sempre que as licenÃ§as de proteÃ§Ã£o ao direito autoral e Ã  propriedade intelectual limitem seu compartilhamento pÃºblico. Nesse caso, o tribunal deverÃ¡ indicar quais sÃ£o os sistemas, motores, bases de dados, LLMs e demais elementos utilizados na soluÃ§Ã£o de IA, acompanhados de suas respectivas</h5>
<p>versÃµes e fornecedores.</p>
<h3>Art. 24. As soluÃ§Ãµes que adotam tÃ©cnicas de inteligÃªncia artificial, tanto em desenvolvimento quanto em uso no Poder JudiciÃ¡rio, deverÃ£o ser cadastradas no Sinapses, que manterÃ¡ um catÃ¡logo de sistemas de IA no JudiciÃ¡rio brasileiro e organizado conforme a categorizaÃ§Ã£o de risco da soluÃ§Ã£o, na forma do Anexo de ClassificaÃ§Ã£o de Riscos desta ResoluÃ§Ã£o.</h3>
<h4>Â§ 1Âº TambÃ©m deverÃ¡ ser incluÃ­do no Sinapses o sumÃ¡rio pÃºblico da avaliaÃ§Ã£o de impacto algorÃ­tmico a que se refere o art. 14 desta ResoluÃ§Ã£o, quando as soluÃ§Ãµes forem classificadas como de alto risco.</h4>
<h4>Â§ 2Âº O sumÃ¡rio pÃºblico poderÃ¡ omitir dados sensÃ­veis, sigilosos ou protegidos por propriedade intelectual, assegurando a proteÃ§Ã£o da privacidade e da confidencialidade das informaÃ§Ãµes.</h4>
<h4>Â§ 3Âº Para as soluÃ§Ãµes de baixo risco, o cadastro no Sinapses deverÃ¡ ser realizado pelo tribunal responsÃ¡vel antes da entrada em produÃ§Ã£o da soluÃ§Ã£o, com as informaÃ§Ãµes mÃ­nimas necessÃ¡rias, como finalidade, criaÃ§Ã£o prÃ³pria ou colaborativa, se a ferramenta Ã© contratada ou desenvolvida internamente e a descriÃ§Ã£o dos objetivos.</h4>
<h4>Â§ 4Âº Para as soluÃ§Ãµes de alto risco, o cadastro no Sinapses poderÃ¡ ser realizado apÃ³s os estudos preliminares, mas necessariamente antes do inÃ­cio do desenvolvimento.</h4>
<h4>Â§ 5Âº As informaÃ§Ãµes cadastradas deverÃ£o ser complementadas e atualizadas conforme a evoluÃ§Ã£o do desenvolvimento da soluÃ§Ã£o, sendo obrigatÃ³ria a atualizaÃ§Ã£o a cada nova fase ou versÃ£o relevante das soluÃ§Ãµes de alto risco.</h4>
<h4>Â§ 6Âº O CNJ deverÃ¡ prover Ã  Plataforma Sinapses a estrutura necessÃ¡ria para recepcionar os cadastros realizados pelos tribunais, sendo dispensado o depÃ³sito de grandes bases de dados ou de modelos protegidos por propriedade intelectual.</h4>
<h3>Art. 25. O CNJ publicarÃ¡, em Ã¡rea prÃ³pria de seu sÃ­tio na rede mundial de computadores, a relaÃ§Ã£o das aplicaÃ§Ãµes que adotam tÃ©cnicas de inteligÃªncia artificial, desenvolvidas ou utilizadas pelos Ã³rgÃ£os do Poder JudiciÃ¡rio, com descriÃ§Ã£o em linguagem simples e precisa e a indicaÃ§Ã£o do grau de risco respectivo, acompanhada de explicaÃ§Ãµes acessÃ­veis sobre as implicaÃ§Ãµes da classificaÃ§Ã£o de risco.</h3>
<h4>Â§ 1Âº As informaÃ§Ãµes deverÃ£o ser atualizadas periodicamente, com revisÃ£o obrigatÃ³ria a cada 12 (doze) meses ou sempre que houver alteraÃ§Ã£o significativa nas aplicaÃ§Ãµes, seja por evoluÃ§Ã£o do software, mudanÃ§as no grau de risco ou descontinuidade.</h4>
<h4>Â§ 2Âº A relaÃ§Ã£o deverÃ¡ indicar de forma clara os critÃ©rios utilizados para a classificaÃ§Ã£o de risco, bem como qualquer situaÃ§Ã£o de descontinuidade ou suspensÃ£o de uso das aplicaÃ§Ãµes.</h4>
<h4>Â§ 3Âº O CNJ poderÃ¡ retirar do catÃ¡logo aplicaÃ§Ãµes descontinuadas ou suspensas, desde que isso seja comunicado publicamente, com justificativa.</h4>
<h2>CAPÃTULO VIII</h2>
<h3>QUALIDADE E SEGURANÃ‡A</h3>
<h3>Art. 26. Os dados utilizados no processo de desenvolvimento de soluÃ§Ãµes de inteligÃªncia artificial deverÃ£o ser preferencialmente provenientes de fontes pÃºblicas ou governamentais, e serÃ£o objeto de curadoria de qualidade, particularmente quando desenvolvidos internamente, e em qualquer caso, respeitando as diretrizes da Lei Geral de ProteÃ§Ã£o de Dados Pessoais.</h3>
<h4>Â§ 1Âº Consideram-se fontes seguras para a obtenÃ§Ã£o de dados aquelas que possuam mecanismos de validaÃ§Ã£o e curadoria de dados, garantindo a sua precisÃ£o, equilÃ­brio, integridade e confiabilidade. Quando dados de fontes nÃ£o governamentais forem utilizados, deverÃ¡ ser realizada uma verificaÃ§Ã£o rigorosa da qualidade e seguranÃ§a dos dados.</h4>
<h4>Â§ 2Âº A utilizaÃ§Ã£o de dados provenientes de fontes nÃ£o governamentais serÃ¡ permitida em casos em que os dados governamentais forem insuficientes ou inadequados para o objetivo especÃ­fico da soluÃ§Ã£o de inteligÃªncia artificial, desde que esses dados sejam validados conforme os critÃ©rios estabelecidos neste artigo.</h4>
<h4>Â§ 3Âº No caso de soluÃ§Ãµes contratadas pelos tribunais, as fornecedoras de serviÃ§os devem garantir contratualmente o respeito Ã s diretrizes da Lei Geral de ProteÃ§Ã£o de Dados Pessoais.</h4>
<h4>Â§ 4Âº DeverÃ£o ser coletados apenas os dados estritamente necessÃ¡rios ao treinamento, nÃ£o devendo ser mantidos conjuntos de dados sem uso ou controle quanto ao armazenamento.</h4>
<h3>Art. 27. O sistema deverÃ¡ impedir que os dados recebidos sejam alterados antes de sua utilizaÃ§Ã£o no fluxo de desenvolvimento de soluÃ§Ãµes de inteligÃªncia artificial, por meio de mecanismos de controle de versÃµes, tokens e registros para auditoria e monitoramento que garantam a integridade e rastreabilidade dos dados.</h3>
<h4>Â§ 1Âº DeverÃ¡ ser mantida uma cÃ³pia de cada conjunto de dados (dataset) utilizado em versÃµes relevantes dos modelos desenvolvidos, garantindo que os dados possam ser auditados e revisados quando necessÃ¡rio.</h4>
<h4>Â§ 2Âº As cÃ³pias dos datasets deverÃ£o ser armazenadas de forma segura, com a utilizaÃ§Ã£o de criptografia e controle de acesso, conforme as diretrizes da Lei Geral de ProteÃ§Ã£o de Dados Pessoais, para assegurar a proteÃ§Ã£o contra acessos nÃ£o autorizados e demais riscos Ã  seguranÃ§a da informaÃ§Ã£o.</h4>
<h4>Â§ 3Âº Na hipÃ³tese de mostrar-se inviÃ¡vel a manutenÃ§Ã£o por longo prazo de todos os datasets das versÃµes relevantes do sistema, em virtude de suas dimensÃµes, o tribunal poderÃ¡ estabelecer um plano de eliminaÃ§Ã£o desses arquivos, conforme tabela de temporalidade adequada ao impacto algorÃ­tmico da soluÃ§Ã£o, sendo garantida a manutenÃ§Ã£o de dataset anteriormente utilizado por, no mÃ­nimo, um ano apÃ³s sua obsolescÃªncia ou modificaÃ§Ã£o.</h4>
<h3>Art. 28. O armazenamento e a execuÃ§Ã£o das soluÃ§Ãµes de inteligÃªncia artificial, operadas em datacenters prÃ³prios, provedores de serviÃ§o de nuvem ou por meio de APIs (interfaces de programaÃ§Ã£o de aplicaÃ§Ãµes), devem garantir o isolamento dos dados compartilhados pelo tribunal, utilizando mecanismos de seguranÃ§a adequados, como criptografia e segregaÃ§Ã£o de ambientes.</h3>
<h4>Â§ 1Âº O isolamento deverÃ¡ assegurar que os dados do tribunal nÃ£o sejam acessados, manipulados ou utilizados por terceiros sem autorizaÃ§Ã£o, garantindo a privacidade e a seguranÃ§a das informaÃ§Ãµes.</h4>
<h4>Â§ 2Âº Os provedores de serviÃ§os de nuvem e APIs deverÃ£o estar em conformidade com a legislaÃ§Ã£o brasileira, incluindo a Lei Geral de ProteÃ§Ã£o de Dados Pessoais, e adotar as melhores prÃ¡ticas de seguranÃ§a da informaÃ§Ã£o para proteger os dados do tribunal.</h4>
<h4>Â§ 3Âº A utilizaÃ§Ã£o de serviÃ§os de nuvem e APIs para armazenamento, processamento e compartilhamento de dados no Ã¢mbito do Poder JudiciÃ¡rio somente poderÃ¡ ser realizada por provedores que atendam a padrÃµes mÃ­nimos obrigatÃ³rios de seguranÃ§a e privacidade, incluindo:</h4>
<h5>I â€“ conformidade com a Lei Geral de ProteÃ§Ã£o de Dados Pessoais;</h5>
<h5>II â€“ certificaÃ§Ãµes internacionais de seguranÃ§a da informaÃ§Ã£o, conforme as diretrizes do ComitÃª Nacional;</h5>
<h5>III â€“ adoÃ§Ã£o de criptografia robusta para dados em trÃ¢nsito e armazenados; e</h5>
<h5>IV â€“ transparÃªncia na polÃ­tica de retenÃ§Ã£o, tratamento e descarte de dados judiciais.</h5>
<h3>Art. 29. Os dados armazenados no processo de desenvolvimento e execuÃ§Ã£o de soluÃ§Ãµes de inteligÃªncia artificial devem ser protegidos de forma eficaz contra os riscos de destruiÃ§Ã£o, modificaÃ§Ã£o, extravio ou acessos e transmissÃµes nÃ£o autorizados, por meio de medidas tÃ©cnicas e administrativas adequadas.</h3>
<h4>Â§ 1Âº A proteÃ§Ã£o dos dados deve incluir a implementaÃ§Ã£o de criptografia, controle de acesso baseado em permissÃµes, auditorias regulares e monitoramento para identificar e mitigar possÃ­veis ameaÃ§as Ã  seguranÃ§a.</h4>
<h4>Â§ 2Âº As prÃ¡ticas de proteÃ§Ã£o de dados deverÃ£o estar em conformidade com a Lei Geral de ProteÃ§Ã£o de Dados Pessoais e com as normativas de seguranÃ§a da informaÃ§Ã£o aplicÃ¡veis, assegurando a privacidade e a integridade dos dados.</h4>
<h4>Â§ 3Âº O uso de ferramentas de monitoramento contÃ­nuo e proativo e de prevenÃ§Ã£o de incidentes serÃ¡ adotado para garantir uma resposta Ã¡gil a qualquer tentativa de violaÃ§Ã£o da seguranÃ§a dos dados.</h4>
<h3>Art. 30. Nos casos em que o uso de soluÃ§Ãµes de inteligÃªncia artificial se dÃª diretamente por meio de sÃ­tios eletrÃ´nicos, aplicativos ou interfaces de programaÃ§Ã£o de aplicaÃ§Ãµes (APIs) que utilizem os dados compartilhados para alimentar o repositÃ³rio central ou para fins de treinamento ou (re)adequaÃ§Ã£o do modelo, Ã© vedado o compartilhamento de dados custodiados pelo JudiciÃ¡rio, exceto quando esses dados forem anonimizados ou pseudoanonimizados na origem, em conformidade com a Lei Geral de ProteÃ§Ã£o de Dados Pessoais e as melhores prÃ¡ticas de seguranÃ§a de dados.</h3>
<h4>Â§ 1Âº Considera-se anonimizaÃ§Ã£o na origem o processo tÃ©cnico de eliminaÃ§Ã£o da possibilidade de associaÃ§Ã£o, direta ou indireta, entre os dados pessoais e uma pessoa natural identificÃ¡vel, realizado antes que os dados sejam transmitidos ou processados pela soluÃ§Ã£o de IA.</h4>
<h4>Â§ 2Âº DeverÃ£o ser adotados mecanismos de auditoria e controle para verificar e garantir a conformidade das soluÃ§Ãµes de IA com as normas de proteÃ§Ã£o de dados, especialmente no uso de dados para fins de treinamento ou readequaÃ§Ã£o de modelos de inteligÃªncia artificial.</h4>
<h3>Art. 31. O armazenamento e a execuÃ§Ã£o dos modelos de inteligÃªncia artificial deverÃ£o ocorrer em ambientes que atendam a padrÃµes consolidados de seguranÃ§a da informaÃ§Ã£o, na forma deste artigo.</h3>
<h4>ParÃ¡grafo Ãºnico. Consideram-se boas prÃ¡ticas para atendimento ao que dispÃµe o caput deste artigo:</h4>
<h5>I â€“ adoÃ§Ã£o de mecanismos de auditoria periÃ³dica e monitoramento contÃ­nuo para assegurar a conformidade dos ambientes com esses padrÃµes de seguranÃ§a, garantindo a proteÃ§Ã£o adequada contra acessos nÃ£o autorizados, falhas de integridade e outras ameaÃ§as Ã  seguranÃ§a da informaÃ§Ã£o;</h5>
<h5>II â€“ implementaÃ§Ã£o de controles de acesso rigorosos, criptografia de dados em repouso e em trÃ¢nsito e polÃ­ticas de gerenciamento de vulnerabilidades nos ambientes de armazenamento e execuÃ§Ã£o; e</h5>
<h5>III â€“ instituiÃ§Ã£o de polÃ­tica de governanÃ§a de dados que busque:</h5>
<h6>a) educar continuamente a equipe sobre prÃ¡ticas de seguranÃ§a da informaÃ§Ã£o, proteÃ§Ã£o de dados pessoais e privacidade;</h6>
<h6>b) ao final do treinamento dos modelos, eliminar os dados pessoais nÃ£o-anonimizados dos repositÃ³rios de dados (data lake, data warehouse ou data lakehouse), observados o Â§ 4Âº do art. 26 e o Â§ 3Âº do art. 27 desta ResoluÃ§Ã£o;</h6>
<h6>c) manter apenas os dados tokenizados estritamente necessÃ¡rios ao modelo, fazendo a guarda dos Ãºltimos datasets aprovados em local que observe a seguranÃ§a da informaÃ§Ã£o, observados o Â§ 4Âº do art. 26 e o Â§ 3Âº do art. 27 desta ResoluÃ§Ã£o;</h6>
<h6>d) implementar a governanÃ§a e curadoria dos dados utilizados, para garantir sua qualidade e seguranÃ§a;</h6>
<h6>e) realizar monitoramento contÃ­nuo e eventualmente auditorias nos modelos em testes e aprovados para garantir a obediÃªncia aos padrÃµes de seguranÃ§a, proteÃ§Ã£o de dados pessoais e privacidade; e</h6>
<h6>f) garantir que modelos fiquem funcionais durante todo o ciclo de vida das soluÃ§Ãµes de IA, removendo-os quando se identifique sua inutilidade ou obsolescÃªncia.</h6>
<h5>IV â€“ adoÃ§Ã£o como referÃªncia, tanto quanto possÃ­vel, de normas internacionais reconhecidas, tais como a ISO/IEC (OrganizaÃ§Ã£o Internacional de PadronizaÃ§Ã£o/ComissÃ£o EletrotÃ©cnica Internacional) 42001, a sÃ©rie ISO/IEC 27000 e as do NIST (NationalInstituteof Standards and Technology), ou as que vierem a sucedÃª-las, alÃ©m das regulamentaÃ§Ãµes locais aplicÃ¡veis.</h5>
<h2>CAPÃTULO IX</h2>
<h3>DO CONTROLE DO USUÃRIO</h3>
<h3>Art. 32. O sistema inteligente deverÃ¡ assegurar a autonomia dos usuÃ¡rios internos, com o uso de modelos que:</h3>
<h5>I â€“ promovam o incremento da eficiÃªncia, precisÃ£o e qualidade das atividades, sem limitar a capacidade de atuaÃ§Ã£o dos usuÃ¡rios;</h5>
<h5>II â€“ possibilitem a revisÃ£o detalhada do conteÃºdo gerado e dos dados utilizados para sua elaboraÃ§Ã£o, assegurando que os usuÃ¡rios tenham acesso Ã s premissas e ao mÃ©todo empregado pela inteligÃªncia artificial na sua formulaÃ§Ã£o, sem que haja qualquer espÃ©cie de vinculaÃ§Ã£o Ã  soluÃ§Ã£o apresentada pela inteligÃªncia artificial e garantindo-se a possibilidade de correÃ§Ãµes ou ajustes.</h5>
<h4>ParÃ¡grafo Ãºnico. Em nenhum momento o sistema de IA poderÃ¡ restringir ou substituir a autoridade final dos usuÃ¡rios internos.</h4>
<h3>Art. 33. Os usuÃ¡rios externos deverÃ£o ser informados, de maneira clara, acessÃ­vel e objetiva, sobre a utilizaÃ§Ã£o de sistemas baseados em IA nos serviÃ§os que lhes forem prestados, devendo ser empregada linguagem simples, que possibilite a fÃ¡cil compreensÃ£o por parte de pessoas nÃ£o especializadas.</h3>
<h4>Â§ 1Âº A informaÃ§Ã£o prevista no caput deste artigo deverÃ¡ destacar o carÃ¡ter consultivo e nÃ£o vinculante da proposta de soluÃ§Ã£o apresentada pela inteligÃªncia artificial, a qual sempre serÃ¡ submetida Ã  anÃ¡lise e decisÃ£o final de uma autoridade competente, que exercerÃ¡ a supervisÃ£o humana sobre o caso.</h4>
<h4>Â§ 2Âº A comunicaÃ§Ã£o sobre o uso de IA deverÃ¡ ser realizada por meio de canais adequados, como avisos nos sistemas utilizados, materiais informativos e guias explicativos, com o intuito de orientar os usuÃ¡rios externos sobre o funcionamento, limitaÃ§Ãµes e objetivos dos sistemas inteligentes no JudiciÃ¡rio.</h4>
<h4>Â§ 3Âº A comunicaÃ§Ã£o sobre o eventual uso da IA no texto de decisÃµes judiciais serÃ¡ uma faculdade de seu signatÃ¡rio, observado o disposto no inciso IV do Â§ 3Âº e o Â§ 6Âº do art. 19 desta ResoluÃ§Ã£o.</h4>
<h4>Â§ 4Âº Os tribunais deverÃ£o disponibilizar periodicamente materiais educativos que ajudem os usuÃ¡rios externos a compreenderem o uso de IA nos processosjudiciais, esclarecendo que tais sistemas tÃªm papel de suporte, sem substituir a autoridade decisÃ³ria humana.</h4>
<h3>Art. 34. Os sistemas computacionais utilizados no Ã¢mbito do Poder JudiciÃ¡rio deverÃ£o exigir a supervisÃ£o humana e permitir a modificaÃ§Ã£o pelo magistrado competente de qualquer produto gerado pela inteligÃªncia artificial, sempre que cabÃ­vel, observado o art. 32 desta ResoluÃ§Ã£o.</h3>
<h2>CAPÃTULO X</h2>
<p>DA PESQUISA, DO DESENVOLVIMENTO E DA IMPLANTAÃ‡ÃƒO DE SERVIÃ‡OS DE INTELIGÃŠNCIA ARTIFICIAL</p>
<h3>Art. 35. A composiÃ§Ã£o de equipes para pesquisa, desenvolvimento e implantaÃ§Ã£o das soluÃ§Ãµes computacionais que se utilizem de inteligÃªncia artificial serÃ¡ orientada pela busca da diversidade e representatividade, com Ãªnfase na inclusÃ£o, sempre que possÃ­vel, de diferentes perfis de gÃªnero e etnia e pessoas com deficiÃªncia, bem como de experiÃªncias e formaÃ§Ã£o em Ã¡reas de conhecimento diversas.</h3>
<h4>Â§ 1Âº A participaÃ§Ã£o representativa deverÃ¡ ser assegurada, tanto quanto possÃ­vel, nas etapas de planejamento, coleta e processamento de dados, construÃ§Ã£o, verificaÃ§Ã£o, validaÃ§Ã£o e implementaÃ§Ã£o dos modelos, tanto nas Ã¡reas tÃ©cnicas como negociais.</h4>
<h4>Â§ 2Âº A diversidade na participaÃ§Ã£o prevista no caput deste artigo poderÃ¡ ser dispensada mediante decisÃ£o fundamentada, dentre outros motivos, pela ausÃªncia de profissionais no quadro de pessoal dos tribunais ou a necessidade de garantir eficÃ¡cia e a velocidade na implementaÃ§Ã£o das soluÃ§Ãµes a curto prazo.</h4>
<h4>Â§ 3Âº A formaÃ§Ã£o das equipes mencionadas no caput deverÃ¡ ter carÃ¡ter interdisciplinar, incluindo profissionais de Tecnologia da InformaÃ§Ã£o, do Direito e de outras Ã¡reas relevantes, cujo conhecimento cientÃ­fico possa contribuir para pesquisa, desenvolvimento ou implantaÃ§Ã£o do sistema inteligente no tribunal.</h4>
<h3>Art. 36. A realizaÃ§Ã£o de estudos, pesquisas, ensino e treinamentos de inteligÃªncia artificial deve ser livre de preconceitos, devendo para tanto:</h3>
<h5>I â€“ respeitar a dignidade e a liberdade de pessoas ou grupos envolvidos em suas atividades, evitando prÃ¡ticas de discriminaÃ§Ã£o, assÃ©dio ou exclusÃ£o;</h5>
<h5>II â€“ coibir atividades que envolvam qualquer forma de risco ou prejuÃ­zo aos seres humanos, como testes inseguros ou a manipulaÃ§Ã£o de dados sensÃ­veis sem consentimento, ou ainda o uso indiscriminado ou malicioso de dados que possam comprometer a equidade das decisÃµes; e</h5>
<h5>III â€“ identificar e evitar sectarismos ou vieses que possam direcionar o curso da pesquisa ou seus resultados, comprometendo a objetividade ou a imparcialidade dos estudos.</h5>
<h3>Art. 37. ConcluÃ­da a pesquisa e iniciado o desenvolvimento de soluÃ§Ãµes que utilizem modelos de inteligÃªncia artificial, os tribunais deverÃ£o cadastrar a iniciativa no Sinapses, na forma do art. 23 desta ResoluÃ§Ã£o, e velar por sua continuidade enquanto for Ãºtil Ã  execuÃ§Ã£o das suas atividades.</h3>
<h4>Â§ 1Âº As atividades descritas no caput deste artigo serÃ£o encerradas quando, mediante manifestaÃ§Ã£o fundamentada, for reconhecida sua desconformidade com os preceitos estabelecidos nesta ResoluÃ§Ã£o ou em outros atos normativos aplicÃ¡veis ao Poder JudiciÃ¡rio e for inviÃ¡vel sua readequaÃ§Ã£o.</h4>
<h4>Â§ 2Âº A utilizaÃ§Ã£o de modelos de inteligÃªncia artificial que empreguem tÃ©cnicas de reconhecimento facial ou de anÃ¡lise biomÃ©trica que configurem aplicaÃ§Ãµes de alto risco, nos termos do Anexo de ClassificaÃ§Ã£o de Risco, item AR5, requererÃ¡ autorizaÃ§Ã£o prÃ©via do ComitÃª Nacional de InteligÃªncia Artificial do JudiciÃ¡rio para o seu desenvolvimento e implementaÃ§Ã£o, sendo imprescindÃ­vel a apresentaÃ§Ã£o de um plano quecomprove a conformidade com os direitos fundamentais, a proteÃ§Ã£o de dados pessoais e o tratamento de potenciais vieses discriminatÃ³rios, em especial quanto Ã  raÃ§a, condiÃ§Ã£o social ou localidade geogrÃ¡fica de moradia.</h4>
<h3>Art. 38. Os modelos de inteligÃªncia artificial poderÃ£o utilizar ferramentas de mercado ou soluÃ§Ãµes de cÃ³digo aberto que:</h3>
<h5>I â€“ facilitem sua integraÃ§Ã£o ou interoperabilidade entre os sistemas utilizados pelos Ã³rgÃ£os do Poder JudiciÃ¡rio, permitindo uma troca de informaÃ§Ãµes eficiente e segura;</h5>
<h5>II â€“ possibilitem um ambiente de desenvolvimento colaborativo, no qual diferentes tribunais e instituiÃ§Ãµes possam contribuir para evoluÃ§Ã£o das soluÃ§Ãµes adequadas;</h5>
<h5>III â€“ permitam maior transparÃªncia, garantindo que os processos e algoritmos utilizados sejam acessÃ­veis para auditoria, monitoramento e revisÃ£o por parte de especialistas autorizados ou por meio da sociedade civil, mediante requerimento;</h5>
<h5>IV â€“ proporcionem cooperaÃ§Ã£o entre outros segmentos e Ã¡reas do setor pÃºblico e a sociedade civil, promovendo iniciativas conjuntas para o desenvolvimento e a implementaÃ§Ã£o de soluÃ§Ãµes de inteligÃªncia artificial;</h5>
<h5>V â€“ assegurem a proteÃ§Ã£o e a seguranÃ§a dos dados utilizados, em particular os dados por cuja guarda o Poder JudiciÃ¡rio seja responsÃ¡vel, adotando medidas que previnam acessos nÃ£o autorizados e preservem a integridade das informaÃ§Ãµes; e</h5>
<h5>VI â€“ garantam a nÃ£o-dependÃªncia tecnolÃ³gica.</h5>
<h2>CAPÃTULO XI</h2>
<h3>DA AUDITORIA E DO MONITORAMENTO</h3>
<h3>Art. 39. Qualquer soluÃ§Ã£o computacional do Poder JudiciÃ¡rio que utilize modelos de inteligÃªncia artificial deverÃ¡ assegurar total transparÃªncia na prestaÃ§Ã£o de contas, com o objetivo de garantir um impacto positivo para os usuÃ¡rios finais e para a sociedade.</h3>
<h4>Â§ 1Âº A prestaÃ§Ã£o de contas compreenderÃ¡:</h4>
<h5>I â€“ os nomes dos responsÃ¡veis pela execuÃ§Ã£o das aÃ§Ãµes e pela prestaÃ§Ã£o de contas;</h5>
<h5>II â€“ os custos envolvidos na pesquisa, desenvolvimento, implantaÃ§Ã£o, comunicaÃ§Ã£o e treinamento;</h5>
<h5>III â€“ a existÃªncia de aÃ§Ãµes de colaboraÃ§Ã£o e cooperaÃ§Ã£o entre os agentes do setor pÃºblico ou entre esses e a iniciativa privada ou a sociedade civil;</h5>
<h5>IV â€“ os resultados pretendidos e os que foram efetivamente alcanÃ§ados;</h5>
<h5>V â€“ a demonstraÃ§Ã£o de efetiva publicidade quanto Ã  natureza do serviÃ§o oferecido, tÃ©cnicas utilizadas, desempenho do sistema e riscos de erros; e</h5>
<h5>VI â€“ a demonstraÃ§Ã£o da divulgaÃ§Ã£o das informaÃ§Ãµes acima mencionadas em formato acessÃ­vel e linguagem simples, atravÃ©s de canais adequados, com atualizaÃ§Ãµes regulares, permitindo a interaÃ§Ã£o do pÃºblico para esclarecimento de dÃºvidas e sugestÃµes.</h5>
<h4>Â§ 2Âº A prestaÃ§Ã£o de contas deverÃ¡ ser publicada em canal oficial e poderÃ¡ ser submetida a auditoria externa por decisÃ£o do Tribunal ou do ComitÃª Nacional de InteligÃªncia Artificial do JudiciÃ¡rio, quando for o caso.</h4>
<h3>Art. 40. O desenvolvimento ou a utilizaÃ§Ã£o de sistemas inteligentes em desacordo com os princÃ­pios e regras estabelecidos nesta ResoluÃ§Ã£o e nos demais normativos aplicÃ¡veis serÃ¡ monitorado, sem carÃ¡ter disciplinar, por parte do ComitÃª Nacional de InteligÃªncia Artificial do JudiciÃ¡rio.</h3>
<h4>ParÃ¡grafo Ãºnico. O monitoramento poderÃ¡ indicar necessidade de auditoria sobre prÃ¡ticas inadequadas, uso indevido de dados e falta de transparÃªncia, e as desconformidades ou discrepÃ¢ncias eventualmente identificadas poderÃ£o ser comunicadas pelo ComitÃª ao Ã³rgÃ£o competente para adoÃ§Ã£o de providÃªncias.</h4>
<h3>Art. 41. O ComitÃª Nacional de InteligÃªncia Artificial do JudiciÃ¡rio estabelecerÃ¡ protocolo de auditoria e monitoramento para modelos e soluÃ§Ãµes de inteligÃªncia artificial em uso no Poder JudiciÃ¡rio.</h3>
<h4>Â§ 1Âº A definiÃ§Ã£o da metodologia para a conduÃ§Ã£o de auditorias serÃ¡ realizada pelo ComitÃª, levando em consideraÃ§Ã£o a identificaÃ§Ã£o dos riscos envolvidos, a definiÃ§Ã£o de salvaguardas (medidas de proteÃ§Ã£o) e a documentaÃ§Ã£o produzida.</h4>
<h4>Â§ 2Âº Para execuÃ§Ã£o das atividades de auditoria e inspeÃ§Ã£o, o ComitÃª poderÃ¡ propor Ã  PresidÃªncia do CNJ a criaÃ§Ã£o de comissÃµes tÃ©cnicas ou grupos de trabalho, que deverÃ£o contar com membros qualificados e com experiÃªncia nas Ã¡reas relacionadas Ã  auditoria de inteligÃªncia artificial.</h4>
<h4>Â§ 3Âº O monitoramento consistirÃ¡ em um conjunto simplificado de anÃ¡lise, verificaÃ§Ã£o e adoÃ§Ã£o de boas prÃ¡ticas de gestÃ£o de dados, processos e produtos, a fim de verificar a regularidade do funcionamento da soluÃ§Ã£o baseada em IA e a manutenÃ§Ã£o de sua conformidade com as diretrizes desta ResoluÃ§Ã£o.</h4>
<h4>Â§ 4Âº Havendo identificaÃ§Ã£o de desconformidades, o ComitÃª fixarÃ¡ prazo para correÃ§Ã£o, que serÃ¡ definido com base na gravidade e impactos da desconformidade.</h4>
<h3>Art. 42. Os Ã³rgÃ£os do Poder JudiciÃ¡rio deverÃ£o informar ao ComitÃª Nacional de InteligÃªncia Artificial do JudiciÃ¡rio todos os eventos adversos relacionados ao uso de soluÃ§Ãµes de inteligÃªncia artificial.</h3>
<h4>Â§ 1Âº Consideram-se eventos adversos os incidentes que resultem em impactos negativos sobre a operaÃ§Ã£o do sistema, a seguranÃ§a dos dados ou a prestaÃ§Ã£o de serviÃ§os.</h4>
<h4>Â§ 2Âº A comunicaÃ§Ã£o dos eventos adversos deverÃ¡ ser realizada no prazo de atÃ© 72 (setenta e duas) horas apÃ³s a sua identificaÃ§Ã£o, contendo descriÃ§Ã£o do incidente, suas causas e as medidas adotadas para correÃ§Ã£o.</h4>
<h4>Â§ 3Âº O ComitÃª analisarÃ¡ as informaÃ§Ãµes recebidas e poderÃ¡ recomendar aÃ§Ãµes corretivas, conforme necessÃ¡rio.</h4>
<h2>CAPÃTULO XII</h2>
<h3>DAS DISPOSIÃ‡Ã•ES FINAIS</h3>
<h3>Art. 43. Os Ã³rgÃ£os do Poder JudiciÃ¡rio poderÃ£o realizar cooperaÃ§Ã£o tÃ©cnica com outras instituiÃ§Ãµes, pÃºblicas ou privadas, ou com a sociedade civil, para o desenvolvimento colaborativo de modelos de inteligÃªncia artificial, desde que observadas as disposiÃ§Ãµes contidas nesta ResoluÃ§Ã£o.</h3>
<h4>Â§ 1Âº A cooperaÃ§Ã£o tÃ©cnica deve incluir a elaboraÃ§Ã£o de acordos que especifiquem as responsabilidades de cada parte no que diz respeito Ã  proteÃ§Ã£o de dados e Ã  confidencialidade das informaÃ§Ãµes compartilhadas.</h4>
<h4>Â§ 2Âº As instituiÃ§Ãµes parceiras devem garantir que os dados utilizados na colaboraÃ§Ã£o atendam aos requisitos da Lei Geral de ProteÃ§Ã£o de Dados Pessoais e Ã s normas de seguranÃ§a estabelecidas pelo Conselho Nacional de JustiÃ§a.</h4>
<h4>Â§ 3Âº As soluÃ§Ãµes de IA do JudiciÃ¡rio devem ser desenvolvidas com a perspectiva de disponibilizaÃ§Ã£o de seus aplicativos na PDPJ-Br, ainda que por meio de versÃ£o adaptada para as peculiaridades tÃ©cnicas da Plataforma.</h4>
<h3>Art. 44. As normas previstas nesta ResoluÃ§Ã£o nÃ£o excluem a aplicaÃ§Ã£o de outras normas do ordenamento jurÃ­dico brasileiro, incluindo, mas nÃ£o se limitando a leis federais, estaduais e municipais, assim como tratados e convenÃ§Ãµes internacionais ratificados pela RepÃºblica Federativa do Brasil.</h3>
<h3>Art. 45. As disposiÃ§Ãµes desta ResoluÃ§Ã£o aplicam-se tambÃ©m aos projetos e modelos de inteligÃªncia artificial jÃ¡ em desenvolvimento ou implantados nos tribunais, respeitados os atos jÃ¡ consolidados.</h3>
<h4>ParÃ¡grafo Ãºnico. Os tribunais terÃ£o um prazo de 12 (doze) meses para adequar seus projetos e modelos, em desenvolvimento ou jÃ¡ implantados, Ã s novas disposiÃ§Ãµes estabelecidas nesta ResoluÃ§Ã£o, a partir de sua publicaÃ§Ã£o.</h4>
<h3>Art. 46. Revoga-se a ResoluÃ§Ã£o CNJ nÂº 332/2020 , a partir do inÃ­cio da vigÃªncia desta ResoluÃ§Ã£o.</h3>
<h3>Art. 47. Esta ResoluÃ§Ã£o entra em vigor apÃ³s decorridos 120 (cento e vinte) dias da data de sua publicaÃ§Ã£o.</h3>
<p>Ministro LuÃ­s Roberto Barroso</p>
<p>ANEXO DA RESOLUÃ‡ÃƒO NÂº 615, DE 11 DE MARÃ‡O DE 2025.</p>
<h3>CLASSIFICAÃ‡ÃƒO DE RISCOS</h3>
<p>Consideram-se de alto risco as seguintes finalidades e contextos para o desenvolvimento de soluÃ§Ãµes baseadas em inteligÃªncia artificial destinadas a desempenhar ou apoiar o usuÃ¡rio na realizaÃ§Ã£o das seguintes atividades acessÃ³rias:</p>
<p>AR1 â€“ identificaÃ§Ã£o de perfis e de padrÃµes comportamentais de pessoas naturais ou de grupos de pessoas naturais, exceto quando enquadradas como situaÃ§Ãµes de risco mÃ­nimo ou controlado, conforme critÃ©rios objetivos estabelecidos;</p>
<p>AR2 â€“ aferiÃ§Ã£o da adequaÃ§Ã£o dos meios de prova e a sua valoraÃ§Ã£o nos processos de jurisdiÃ§Ã£o contenciosa, sejam documentais, testemunhais, periciais ou de outras naturezas, especialmente quando tais avaliaÃ§Ãµes possam influenciar diretamente a decisÃ£o judicial;</p>
<p>AR3 â€“ averiguaÃ§Ã£o, valoraÃ§Ã£o, tipificaÃ§Ã£o e a interpretaÃ§Ã£o de fatos como sendo crimes, contravenÃ§Ãµes penais ou atos infracionais, ressalvadas as soluÃ§Ãµes voltadas Ã  mera rotina da execuÃ§Ã£o penal e de medidas socioeducativas;</p>
<p>AR4 â€“ formulaÃ§Ã£o de juÃ­zos conclusivos sobre a aplicaÃ§Ã£o da norma jurÃ­dica ou precedentes a um conjunto determinado de fatos concretos, inclusive para a quantificaÃ§Ã£o ou a qualificaÃ§Ã£o de danos suportados por pessoas ou grupos, em aÃ§Ãµes criminais ou nÃ£o;</p>
<p>AR5 â€“ identificaÃ§Ã£o e a autenticaÃ§Ã£o facial ou biomÃ©trica para o monitoramento de comportamento de pessoas naturais, exceto quando utilizada para a mera confirmaÃ§Ã£o da identidade de uma pessoa natural especÃ­fica ou para atividades de seguranÃ§a pÃºblica devidamente justificadas, sempre garantida a observÃ¢ncia dos direitos fundamentais e monitoramento contÃ­nuo de tais soluÃ§Ãµes.</p>
<p>Consideram-se de baixo risco as seguintes finalidades e contextos para o desenvolvimento de soluÃ§Ãµes baseadas em inteligÃªncia artificial destinadas a desempenhar ou apoiar o usuÃ¡rio na realizaÃ§Ã£o das seguintes atividades acessÃ³rias:</p>
<p>BR1 â€“ execuÃ§Ã£o de atos processuais ordinatÃ³rios ou de tarefas de apoio Ã  administraÃ§Ã£o judiciÃ¡ria, mediante a extraÃ§Ã£o de informaÃ§Ãµes de sistemas e de documentos, com a finalidade de classificaÃ§Ã£o e agrupamento de dados e processos, enriquecimento de cadastros, certificaÃ§Ã£o e transcriÃ§Ã£o de atos processuais, sumarizaÃ§Ã£o ou resumo de documentos, entre outras finalidades de gestÃ£o processual</p>
<p>e operacional, desde que supervisionadas por responsÃ¡vel humano;</p>
<p>BR2 â€“ detecÃ§Ã£o de padrÃµes decisÃ³rios ou de desvios de padrÃµes decisÃ³rios, bem como detecÃ§Ã£o de precedentes qualificados pertinentes, observado o carÃ¡ter complementar da tÃ©cnica de inteligÃªncia artificial, desde que nÃ£o haja substituiÃ§Ã£o da avaliaÃ§Ã£o humana sobre</p>
<p>processos, sendo seu uso destinado para apoio interno ao tribunal e para uniformizaÃ§Ã£o da jurisprudÃªncia;</p>
<p>BR3 â€“ fornecimento aos magistrados de subsÃ­dios para a tomada de decisÃ£o mediante relatÃ³rios gerenciais e anÃ¡lises que adotem tÃ©cnica jurimÃ©trica, com a integraÃ§Ã£o de fontes de informaÃ§Ã£o relevantes ou a detecÃ§Ã£o de padrÃµes decisÃ³rios, desde que nÃ£o haja substituiÃ§Ã£o da avaliaÃ§Ã£o humana e que a soluÃ§Ã£o nÃ£o realize valoraÃ§Ãµes de cunho moral sobre provas ou sobre perfis e condutas de pessoas;</p>
<p>BR4 â€“ produÃ§Ã£o de textos de apoio para facilitar a confecÃ§Ã£o de atos judiciais, desde que a supervisÃ£o e a versÃ£o final do documento sejam realizadas pelo magistrado e com base em suas instruÃ§Ãµes, especialmente as decisÃµes acerca das preliminares e questÃµes de mÃ©rito;</p>
<p>BR5 â€“ aprimoramento ou formataÃ§Ã£o de uma atividade humana anteriormente realizada, desde que nÃ£o se altere materialmente o seu resultado, ou ainda realizaÃ§Ã£o de uma tarefa preparatÃ³ria para uma outra, considerada como de alto risco;</p>
<p>BR6 â€“ realizaÃ§Ã£o de anÃ¡lises estatÃ­sticas para fins de polÃ­tica judiciÃ¡ria, sempre com supervisÃ£o humana contÃ­nua, especialmente para evitar conclusÃµes enviesadas;</p>
<p>BR7 â€“ transcriÃ§Ã£o de Ã¡udio e vÃ­deo para o auxÃ­lio das atividades do magistrado, com revisÃ£o final realizada por pessoa responsÃ¡vel;</p>
<p>BR8 â€“ anonimizaÃ§Ã£o de documentos ou de sua exibiÃ§Ã£o, especialmente para garantir sua conformidade com as normas de privacidade e proteÃ§Ã£o de dados.</p>
</article>
<footer>
<p class="disclaimer">
<strong>Aviso Legal:</strong> Este Ã© um espelho nÃ£o oficial para fins tÃ©cnicos de ingestÃ£o por agentes de IA. 
 O conteÃºdo oficial e juridicamente vÃ¡lido estÃ¡ disponÃ­vel no 
 <a href="https://atos.cnj.jus.br/atos/detalhar/6001" rel="nofollow" target="_blank">CNJ â€“ Atos Normativos</a> 
 e no DiÃ¡rio da JustiÃ§a EletrÃ´nico.
 </p>
</footer>
</main>
<!-- Script de Controle de Tema -->
<script>
 (function() {
 // FunÃ§Ã£o para aplicar o tema
 function applyTheme(theme) {
 const root = document.documentElement;
 
 if (theme === 'dark') {
 root.setAttribute('data-theme', 'dark');
 } else if (theme === 'light') {
 root.removeAttribute('data-theme');
 } else if (theme === 'auto') {
 // Detecta o tema do sistema
 const prefersDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
 if (prefersDark) {
 root.setAttribute('data-theme', 'dark');
 } else {
 root.removeAttribute('data-theme');
 }
 }
 }
 
 // FunÃ§Ã£o para atualizar a UI do toggle
 function updateToggleUI(theme) {
 const options = document.querySelectorAll('.theme-option');
 options.forEach(option => {
 const input = option.querySelector('input');
 if (input.value === theme) {
 option.classList.add('active');
 input.checked = true;
 } else {
 option.classList.remove('active');
 input.checked = false;
 }
 });
 }
 
 // Carrega a preferÃªncia salva ou usa 'auto' como padrÃ£o
 const savedTheme = localStorage.getItem('theme') || 'auto';
 applyTheme(savedTheme);
 
 // Aguarda o DOM estar pronto para configurar os event listeners
 document.addEventListener('DOMContentLoaded', function() {
 updateToggleUI(savedTheme);
 
 // Adiciona listeners aos botÃµes de tema
 const themeOptions = document.querySelectorAll('.theme-option');
 themeOptions.forEach(option => {
 option.addEventListener('click', function() {
 const input = this.querySelector('input');
 const theme = input.value;
 
 // Salva a preferÃªncia
 localStorage.setItem('theme', theme);
 
 // Aplica o tema
 applyTheme(theme);
 
 // Atualiza a UI
 updateToggleUI(theme);
 });
 });
 
 // Observa mudanÃ§as no tema do sistema quando 'auto' estÃ¡ selecionado
 const darkModeMediaQuery = window.matchMedia('(prefers-color-scheme: dark)');
 darkModeMediaQuery.addEventListener('change', function(e) {
 const currentTheme = localStorage.getItem('theme') || 'auto';
 if (currentTheme === 'auto') {
 applyTheme('auto');
 }
 });
 });
 })();
 </script>
</body>
</html>